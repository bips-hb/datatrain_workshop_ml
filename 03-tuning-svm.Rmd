---
title: "03: Tuning & SVMs"
date: "`r Sys.time()`"
output: 
  html_notebook: 
    toc: yes
    theme: flatly
    number_sections: yes
editor_options: 
  chunk_output_type: console
---

```{r setup}
library(mlr3verse)   # All the mlr3 things
library(mlr3viz)     # Explicitly for plot_learner_prediction()
library(ggplot2)     # For plotting

# Spam Task setup
spam_task <- tsk("spam")
set.seed(26)
spam_train <- sample(spam_task$nrow, 2/3 * spam_task$nrow)
spam_test <- setdiff(seq_len(spam_task$nrow), spam_train)
```

Goals of this part:

1. Introduce hyperparameter tuning
2. Experiment with tuning different learners
3. Introduce SVMs
4. Tune an SVM with a more complex setup

# Hyperparameter Tuning

So far we've seen four learners:

1. kNN via `{kknn}`
2. Decision Trees via `{rpart}`
3. Random Forest via `{ranger}`
4. Gradient Boosting via `{xgboost}`

We've gotten to know the first two a little, and now we'll also take a closer
look at the second two.

First we'll start doing some tuning with `{mlr3}` based on the **kNN** learner
because it's nice and simple.  
We saw that `k` is an important parameter, and it's an integer greater than 1 at least.
To tune it, we also have to make a few other decisions, also using what we learned about
resampling.  

- What's our inner resampling strategy?
- What measure to we tune on? 
- What does the parameter search space look like?
- How long to we tune? What's our *budget*?
- What's our tuning strategy?

We'll use `{mlr3}`'s [`AutoTuner`](https://mlr3book.mlr-org.com/optimization.html#autotuner)
for this because it's just so convenient:

```{r knn-tuning-setup}
# Defining a search space: k is an integer, we look in range 3 to 51
search_space_knn = ps(
  k = p_int(lower = 3, upper = 51)
)

tuned_knn = AutoTuner$new(
  # The base learner we want to tune, optionally setting other parameters
  learner = lrn("classif.kknn", predict_type = "prob"),
  # Resampling strategy, here holdout with default split
  resampling = rsmp("holdout"),
  # Tuning measure: Maximize the AUC
  measure = msr("classif.auc"),
  # Setting the search space we defined above
  search_space = search_space_knn,
  # Budget: Try n_evals different values
  terminator = trm("evals", n_evals = 30),
  # Strategy: Randomly try parameter values in the space
  tuner = tnr("random_search")
)

# Take a look at the new tuning learner
tuned_knn
```

Now `tuned_knn` behaves the same way as any other Learner that has not been
trained on any data yet - first we have to train (and tune!) it on our spam training data.
The result will be the best hyperparameter configuration of those we tried:

```{r tune-knn}
# Setting a seed so we get the same result in the workshop
set.seed(2398)
tuned_knn$train(spam_task, row_ids = spam_train)
```

In this case, we get a `k` of 25, with an AUC of about 0.96 in the inner resampling 
(the holdout set during tuning).

We can visualize the performance across all the `k`s we tried by accessing the
tuning instance now included in the `tuned_knn` learner object:

```{r plot-knn-tuning-result}
autoplot(tuned_knn$tuning_instance)
```

And we can get the hyperparameter results that worked best in the end:

```{r knn-tuning-param-results}
tuned_knn$tuning_result
```

Now that we've tuned on the training set, it's time to evaluate on the test set, and
to shorten it a little we'll do prediction and scoring in one line:

```{r knn-eval}
tuned_knn$predict(spam_task, row_ids = spam_test)$score(msr("classif.auc"))
```

Seems like a decent result?  
Let's try to beat it with some other learner!

## Your Turn!

Above you have a boilerplate to tune your own learner.  
Start with either of the other three learners we've seen, pick one ore two hyperparameters
to tune with a reasonable budget (note we have limited time and resources),
tune on the training set and evaluate per AUC on the test set.

Some pointers:

- Consult the Learner docs to see tuning-worthy parameters:
    - `lrn("classif.xgboost")$help()` links to the `xgboost` help
    - `lrn("classif.rpart")$help()` analogously for the decision tree
    - You can also see the documentation online, e.g. https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.xgboost.html
- Parameter search spaces in `ps()` have different types, see the help at `?paradox::Domain`
    - Use `p_int()` for integers, `p_dbl()` for real-valued params etc.
- If you don't know which parameter to tune, try the following:
    - `classif.xgboost`: 
        - Important: `nrounds` (integer) (>= 1)
        - Important: `eta` (double) (0 < eta < 1)
        - Maybe: `max_depth` (integer) (< 30)
    - `classif.rpart`:
        - `cp` (double)
        - Maybe: `maxdepth` (integer) (< 30)
    - `classiff.ranger`:
        - `num.trees` (integer) (default is 500)
        - `max.depth` (integer) (ranger has no limit here :)

Note: Instead of randomly picking parameters from the design space, we can also
generate a grid of parameters and try those.  
We'll not try that here for now, but you can read up on how to do that here:
`?mlr_tuners_grid_search`.


# Support Vector Machines

Let's circle back to new learners and explore SVMs a little by trying
out different kernels at the example of our penguin dataset we used in the beginning:

```{r penguins}
penguins <- na.omit(palmerpenguins::penguins)

ggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +
  geom_point() +
  labs(
    title = "Palmer Penguins",
    x = "Flipper Length [mm]", y = "Body Mass [g]",
    color = "Species"
  ) +
  theme_minimal()
```

Since we don't care about prediction accuracy for now, we'll use the whole dataset
for training and prediction. Please only do this with toy data ðŸ™ƒ.

For the SVM algorithm itself, we use the `svm` learner from the `e1071` package (great name, I know)
but once again use `{mlr3}`'s interface

According to the docs (`?e1071::svm`) we have the choice of the following kernels:

- `"linear"`: $u'v$
- `"polynomial"`: $(\mathtt{gamma} \cdot u' \cdot v + \mathtt{coef0})^\mathtt{degree}$
- `"radial"`: $\exp(-\mathtt{gamma} \cdot |u-v|^2)$
- `"sigmoid"`: $\tanh(\mathtt{gamma} \cdot u'v + \mathtt{coef0})$

Where `gamma`, `degree`, and `coef0` are further hyperparameters.

```{r svm-learner-default}
svm_learner <- lrn("classif.svm")

# What parameters do we have?
svm_learner$param_set$ids()

# Default kernel
svm_learner$param_set$default$kernel
```


## Your Turn!

Below you have a boilerplate for

a) Creating an SVM learner and train it on the penguin dataset with 2 predictors
b) Plotting decision boundaries with it (using the `{mlr3}` helper function)

Run the code below once to see what linear decision boundaries look like, then
pick different kernels from the list above and run it again.

- What kernel would you pick just by the looks of the boundaries?
- How do the boundaries change if you also adjust the other hyperparameters?
- Try picking any other two variables as features (`penguin_task$col_info`)

```{r penguin-task-2-predictors}
penguin_task <- TaskClassif$new(
  id = "penguins", 
  backend = na.omit(palmerpenguins::penguins), 
  target = "species"
)

penguin_task$col_roles$feature <- c("body_mass_g", "flipper_length_mm")
```


```{r svm-decision-boundaries}
# Create the learner, picking a kernel and/or other hyperparams
svm_learner <- lrn("classif.svm", kernel = "polynomial", degree = 7)

# Train the learner
svm_learner$train(penguin_task)

# Plot decision boundaries
plot_learner_prediction(
  learner = svm_learner, 
  task = penguin_task
)
```


## SVM-Tuning

Let's try a more complex tuning experiment, based on the spam task from before.

We'll create a new SVM learner object and this time explicitly tell it which
classification to do - that's the default value anyway, but `{mlr3}` wants us
to be explicit here for tuning:

```{r svm-learner}
svm_learner <- lrn("classif.svm", predict_type = "prob", type = "C-classification")
```

First up we'll define our search space, meaning the range of parameters we want to test out.
Since `kernel` is a categorical parameter (i.e. no numbers, just names of kernels),
we'll define the search space for that parameter by just passing the names of
the kernels to the `p_fct()` helper function that defines `factor`-parameters in `{mlr3}`.

The interesting thing here is that some parameters are only relevant for some kernels,
wich we can declare via a `depends` argument:

```{r svm-search-space-short}
search_space_svm = ps(
  kernel = p_fct(c("linear", "polynomial", "radial", "sigmoid")),
  degree = p_int(1, 7, depends = kernel == "polynomial")
)

# Create an example design grid to inspect our setup:
generate_design_grid(search_space_svm, resolution = 3)
```

## Your Turn!

The above should get you started to...

1. Create a `search_space_svm` like above, tuning...
    - `cost` from 0.1 to 1 (hint: `trafo = function(x) 10^x`)
    - `kernel`, (like above example)
    - `degree`, as above, **only if** `kernel == "polynomial"`
    - `gamma`, from e.g. 0.01 to 0.2, **only if** `kernel` is polynomial, radial, sigmoid
        (hint: you can't use `kernel != "linear"` unfortunately, but `kernel %in% c(...)`) works
2. Use the `AutoTuner$new()` function as previously seen with 
    - `svm_learner` (see above)
    - A resampling strategy (use `"holdout"` if runtime is an issue)
    - A measure (e.g. `classif.acc` or `classif.auc`)
    - The search space you created in 1.
    - A termination criterion (e.g. 40 evaluations)
    - Random search as your tuning strategy
3. Train the AutoTuned learner and evaluate on the test set
