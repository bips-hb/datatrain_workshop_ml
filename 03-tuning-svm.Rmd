---
title: "03: Tuning & SVMs"
date: "`r Sys.time()`"
output: 
  html_notebook: 
    toc: yes
    theme: flatly
    number_sections: yes
editor_options: 
  chunk_output_type: console
---

```{r setup}
library(mlr3verse)   # All the mlr3 things
library(ggplot2)     # For plotting

# Spam Task setup
spam_task <- tsk("spam")
set.seed(26)
spam_train <- sample(spam_task$nrow, 2/3 * spam_task$nrow)
spam_test <- setdiff(seq_len(spam_task$nrow), spam_train)
```

# Hyperparameter Tuning

So far we've seen four learners:

1. kNN via `{kknn}`
2. Decision Trees via `{rpart}`
3. Random Forest via `{ranger}`
4. Gradient Boosting via `{xgboost}`

We've gotten to know the first two a little, and now we'll also take a closer
look at the second two.

First we'll start doing some tuning with `{mlr3}` based on the **kNN** learner
because it's nice and simple.  
We saw that `k` is an important parameter, and it's an integer greater than 1 at least.
To tune it, we also have to make a few other decisions, also using what we learned about
resampling.  

- What's our inner resampling strategy?
- What measure to we tune on? 
- What does the parameter search space look like?
- How long to we tune? What's our *budget*?
- What's our tuning strategy?

We'll use `{mlr3}`'s [`AutoTuner`](https://mlr3book.mlr-org.com/optimization.html#autotuner)
for this because it's just so convenient:

```{r knn-tuning-setup}
# Defining a search space: k is an integer, we look in range 3 to 51
search_space = ps(
  k = p_int(lower = 3, upper = 51)
)

tuned_knn = AutoTuner$new(
  # The base learner we want to tune, optionally setting other parameters
  learner = lrn("classif.kknn", predict_type = "prob"),
  # Resampling strategy, here holdout with default split
  resampling = rsmp("holdout"),
  # Tuning measure: Maximize the AUC
  measure = msr("classif.auc"),
  # Setting the search space we defined above
  search_space = search_space,
  # Budget: Try n_evals different values
  terminator = trm("evals", n_evals = 30),
  # Strategy: Randomly try parameter values in the space
  tuner = tnr("random_search")
)

# Take a look at the new tuning learner
tuned_knn
```

Now `tuned_knn` behaves the same way as any other Learner that has not been
trained on any data yet - first we have to train (and tune!) it on our spam training data.
The result will be the best hyperparameter configuration of those we tried:

```{r tune-knn}
# Setting a seed so we get the same result in the workshop
set.seed(2398)
tuned_knn$train(spam_task, row_ids = spam_train)
```

In this case, we get a `k` of 25, with an AUC of about 0.96 in the inner resampling 
(the holdout set during tuning).

We can visualize the performance across all the `k`s we tried by accessing the
tuning instance now included in the `tuned_knn` learner object:

```{r plot-knn-tuning-result}
autoplot(tuned_knn$tuning_instance)
```


Now that we've tuned on the training set, it's time to evaluate on the test set, and
to shorten it a little we'll do prediction and scoring in one line:

```{r knn-eval}
tuned_knn$predict(spam_task, row_ids = spam_test)$score(msr("classif.auc"))
```

Seems like a decent result?  
Let's try to beat it with some other learner!

## Your Turn!

Above you have a boilerplate to tune your own learner.  
Start with either of the other three learners we've seen, pick one ore two hyperparameters
to tune with a reasonable budget (note we have limited time and resources),
tune on the training set and evaluate per AUC on the test set.

Some pointers:

- Consult the Learner docs to see tuning-worthy parameters:
    - `lrn("classif.xgboost")$help()` links to the `xgboost` help
    - `lrn("classif.rpart")$help()` analogously for the decision tree
    - You can also see the documentation online, e.g. https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.xgboost.html
- Parameter search spaces in `ps()` have different types, see the help at `?paradox::Domain`
    - Use `p_int()` for integers, `p_dbl()` for real-valued params etc.
- If you don't know which parameter to tune, try the following:
    - `classif.xgboost`: 
        - Important: `nrounds` (integer) (>= 1)
        - Important: `eta` (double) (0 < eta < 1)
        - Maybe: `max_depth` (integer) (< 30)
    - `classif.rpart`:
        - `cp` (double)
        - Maybe: `maxdepth` (integer) (< 30)
    - `classiff.ranger`:
        - `num.trees` (integer) (default is 500)
        - `max.depth` (integer) (ranger has no limit here :)

Note: Instead of randomly picking parameters from the design space, we can also
generate a grid of parameters and try those. See `?mlr_tuners_grid_search`.


### Example Code

```{r tuned-rpart}
# Tuning setup
tuned_rpart = AutoTuner$new(
  learner = lrn("classif.rpart", predict_type = "prob"),
  resampling = rsmp("holdout"),
  measure = msr("classif.auc"),
  search_space = ps(
    cp = p_dbl(lower = 0.001, upper = 0.03),
    maxdepth = p_int(lower = 1, upper = 30)
  ),
  terminator = trm("evals", n_evals = 40),
  tuner = tnr("random_search")
)

# Tune!
tuned_rpart$train(spam_task, row_ids = spam_train)

# Evaluate!
tuned_rpart$predict(spam_task, row_ids = spam_test)$score(msr("classif.auc"))

# Check parameter results
autoplot(tuned_rpart$tuning_instance)
```

```{r tuned-xgboost}
# Tuning setup
tuned_xgboost = AutoTuner$new(
  learner = lrn("classif.xgboost", predict_type = "prob"),
  resampling = rsmp("holdout"),
  measure = msr("classif.auc"),
  search_space = ps(
    eta = p_dbl(lower = 0.001, upper = 1),
    nrounds = p_int(lower = 1, upper = 20)
  ),
  terminator = trm("evals", n_evals = 50),
  tuner = tnr("random_search")
)

# Tune!
tuned_xgboost$train(spam_task, row_ids = spam_train)

autoplot(tuned_xgboost$tuning_instance, cols_x = c("nrounds", "eta"))

# Evaluate!
tuned_xgboost$predict(spam_task, row_ids = spam_test)$score(msr("classif.auc"))

# Check parameter results
autoplot(tuned_xgboost$tuning_instance)
```


```{r tuned-ranger}
# Tuning setup
tuned_ranger = AutoTuner$new(
  learner = lrn("classif.ranger", predict_type = "prob"),
  resampling = rsmp("holdout"),
  measure = msr("classif.auc"),
  search_space = ps(
    num.trees = p_int(lower = 200, upper = 700)
  ),
  terminator = trm("evals", n_evals = 20),
  tuner = tnr("random_search")
)

# Tune!
tuned_ranger$train(spam_task, row_ids = spam_train)

# Evaluate!
tuned_ranger$predict(spam_task, row_ids = spam_test)$score(msr("classif.auc"))

# Check parameter results
autoplot(tuned_ranger$tuning_instance)
```

# Support Vector Machines

Let's circle back to new learners and explore SVMs a little by trying
out different kernels at the example of our penguin dataset we used in the beginning:

```{r penguins}
penguins <- na.omit(palmerpenguins::penguins)

ggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +
  geom_point() +
  labs(
    title = "Palmer Penguins",
    x = "Flipper Length [mm]", y = "Body Mass [g]",
    color = "Species"
  )
```

Since we don't care about prediction accuracy for now, we'll use the whole dataset
for training and prediction. Please only do this with toy data ðŸ™ƒ.

For the SVM algorithm itself, we use `svm()` from the `e1071` package (great name, I know)
but once again use mlr3's interface

According to the docs (`?e1071::svm`) we have the choice of the following kernels:

- `"linear"`: $u'v$
- `"polynomial"`: $(\mathtt{gamma} \cdot u' \cdot v + \mathtt{coef0})^\mathtt{degree}$
- `"radial"`: $\exp(-\mathtt{gamma} \cdot |u-v|^2)$
- `"sigmoid"`: $\tanh(\mathtt{gamma} \cdot u'v + \mathtt{coef0})$

Where `gamma`, `degree`, and `coef0` are further hyperparameters.

```{r svm-learner-default}
svm_learner <- lrn("classif.svm")

# What parameters do we have?
svm_learner$param_set$ids()

# Default kernel
svm_learner$param_set$default$kernel
```


## Your Turn!

Below you have a boilerplate for

a) Creating an SVM learner and train it on the penguin dataset with 2 predictors
b) Plotting decision boundaries with it (using the mlr3 helper function)

Run the code below once to see what linear decision boundaries look like, then
pick different kernels from the list above and run it again.

- What kernel would you pick just by the looks of the boundaries?
- How do the boundaries change if you also adjust the other hyperparameters?
- Try picking any other two variables as features (`penguin_task$col_info`)

```{r penguin-task-2-predictors}
penguin_task <- TaskClassif$new(
  id = "penguins", 
  backend = na.omit(palmerpenguins::penguins), 
  target = "species"
)

penguin_task$col_roles$feature <- c("body_mass_g", "flipper_length_mm")
```


```{r svm-decision-boundaries}
# Create the learner, picking a kernel and/or other hyperparams
svm_learner <- lrn("classif.svm", kernel = "polynomial", degree = 7)

# Train the learner
svm_learner$train(penguin_task)

# Plot decision boundaries
plot_learner_prediction(
  learner = svm_learner, 
  task = penguin_task
)
```


## SVM-Tuning (move/shorten?)

```{r spam-task}
spam_task <- tsk("spam")
```

```{r svm-learner}
# Pick our SVM learner, now with more explicit options
svm_learner <- lrn("classif.svm", predict_type = "prob", type = "C-classification")
```


First up we'll define our search space, meaning the range of parameters we want to test out.
Since `kernel` is a categorical parameter (i.e. no numbers, just names of kernels),
we'll define the search space for that parameter by just passing the names of
the kernels to the `p_fct()` helper function that defines `factor`-parameters in mlr3.

```{r svm-search-space-short}
search_space_svm = ps(
  kernel = p_fct(c("linear", "polynomial", "radial", "sigmoid")),
  degree = p_int(1, 7, depends = kernel == "polynomial")
)

generate_design_grid(search_space_svm, resolution = 3)
```



```{r}
search_space_svm = ps(
  cost = p_dbl(-1, 1, trafo = function(x) 10^x),
  kernel = p_fct(c("linear", "polynomial", "radial", "sigmoid")),
  degree = p_int(1, 7, depends = kernel == "polynomial"),
  gamma = p_dbl(lower = 0.01, upper = 0.2, depends = kernel %in% c("polynomial", "radial", "sigmoid"))
)

generate_design_grid(search_space_svm, resolution = 6)
```

```{r}
tuned_svm = AutoTuner$new(
  learner = svm_learner,
  resampling = rsmp("holdout"),
  measure = msr("classif.auc"),
  search_space = search_space_svm,
  terminator = trm("evals", n_evals = 50),
  tuner = tnr("random_search")
)

# Tune!
set.seed(313)
tuned_svm$train(spam_task, row_ids = spam_train)

# Evaluate!
tuned_svm$predict(spam_task, row_ids = spam_test)$score(msr("classif.auc"))
```

## Bonus Feature: SVM Penguin Tuning

```{r}
search_space_svm = ps(
  cost = p_dbl(-1, 1, trafo = function(x) 10^x),
  kernel = p_fct(c("linear", "polynomial", "radial", "sigmoid")),
  degree = p_int(1, 7, depends = kernel == "polynomial"),
  gamma = p_dbl(lower = 0.01, upper = 0.2, depends = kernel %in% c("polynomial", "radial", "sigmoid"))
)

tuned_svm_penguins = AutoTuner$new(
  learner = lrn("classif.svm", type = "C-classification"),
  resampling = rsmp("holdout"),
  measure = msr("classif.ce"),
  search_space = search_space_svm,
  terminator = trm("evals", n_evals = 20),
  tuner = tnr("random_search")
)

# Tune on penguins
penguin_task <- TaskClassif$new(
  id = "penguins", 
  backend = na.omit(palmerpenguins::penguins), 
  target = "species"
)
penguin_task$set_col_roles(cols = c("island", "sex", "year", "bill_depth_mm", "bill_length_mm"), remove_from = "feature")

set.seed(26)
penguin_train <- sample(penguin_task$nrow, 2/3 * penguin_task$nrow)
penguin_test <- setdiff(seq_len(penguin_task$nrow), penguin_train)

tuned_svm_penguins$train(penguin_task, row_ids = penguin_train)

# Evaluate!
tuned_svm_penguins$predict(penguin_task, row_ids = penguin_test)$score(msr("classif.acc"))
```

```{r}
# Prediction grid
flipper_range <- range(penguins$flipper_length_mm)
mass_range <- range(penguins$body_mass_g)

pred_grid <- expand.grid(
  flipper_length_mm = seq(flipper_range[1], flipper_range[2], length.out = 200),
  body_mass_g = seq(mass_range[1], mass_range[2], length.out = 200)
)

# Add predictions for the grid
pred_grid$prediction <- tuned_svm_penguins$predict_newdata(newdata = pred_grid)$response

# Plot decision boudnaries
ggplot(pred_grid, aes(x = flipper_length_mm, y = body_mass_g, color = prediction)) +
  geom_point(alpha = .1, shape = 15) +
  geom_point(data = penguins, aes(fill = species), shape = 21, color = "black") +
  labs(
    title = "Palmer Penguins",
    subtitle = "Species predicted by tuned SVM",
    x = "Flipper Length [mm]", y = "Body Mass [g]",
    color = "Species (prediction)", fill = "Species (Observed)"
  )
```

