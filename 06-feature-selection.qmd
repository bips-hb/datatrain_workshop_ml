---
title: "Feature Selection"
editor_options: 
  chunk_output_type: console
---

```{r setup}
#| message: false
library(mlr3verse) # All the mlr3 things
lgr::get_logger("mlr3")$set_threshold("error")

# Spam Task setup
spam_task <- tsk("spam")
```

Goals of this part:

1. Introduce feature selection
2. Introduce the `auto_fselector` analogous to `auto_tuner`

# Feature Selection

There is a lot more to cover than we have time for here, see e.g.:

- [mlr3gallery post with example](https://mlr-org.com/gallery/optimization/2020-09-14-mlr3fselect-basic/index.html)
- [mlr3book](https://mlr3book.mlr-org.com/chapters/chapter6/feature_selection.html)

Selecting features with `{mlr3}` is similar to parameter tuning: We need to set a budget (e.g. 20 evaluations like before) and a criterion (like the AUC) with a resampling strategy (here holdout for simplicity).

The feature seelction instance defines our search:

```{r fselect-instance}
fselect_instance = fsi(
  task = spam_task,
  learner = lrn("classif.rpart", predict_type = "prob"),
  resampling = rsmp("holdout"),
  measure = msr("classif.auc"),
  terminator = trm("evals", n_evals = 20)
)

fselect_instance
```

There are multiple feature selection methods available:

- Random Search (`"random_search`): Randomly try combinations of features until our budget is exhausted
- Exhaustive Search (`exhaustive_search`): Try all possible subsets of features. Can take a trillion years. Or 10 minutes
- Sequential Search (`sequential`): Forwards- (default) or backwards-selection
- Recursive Feature Elimination (`rfe`): Recursively eliminates features with low `$importance` score (if the `Learner` supports it!)

```{r mlr-fselectors}
as.data.table(mlr_fselectors)
```

As you might be able to imagine, doing an exhaustive search is not often feasible when we're working with a lot of features. For a dataset with 10 features, examining every possible subset of features would yield over 1000 models to evaluate. 
You can imagine how feasible that approach would be for genome-wide studies with thousands of variables.

Random search it is, then!

```{r fselector-optimize}
fselector <- fs("random_search")

fselector$optimize(fselect_instance)
```

Here we have picked an selection strategy (ultimitaley also just an optimization problem) and used it on our selection problem.

We can look at the results, also similar to tuning results:

```{r fselect-results}
fselect_instance$result_feature_set

fselect_instance$result_y
```

We can also look at the (somewhat unqieldy) tuning archive which shows us all of the feature combinations that we tried out, wehre `TRUE` indicates features that were in this particular evaluation and `FALSE` for those omitted.

```{r fselect-results-long}
as.data.table(fselect_instance$archive)[1:5, ]
```

Similar to the `auto_tuner` we used for parameter tuning, there's also an `auto_fselector` which basically works the same way, giving us a "self-tuning" learner as a result

```{r auto-fselect}
fselected_rpart <- auto_fselector(
  learner = lrn("classif.rpart", predict_type = "prob"),
  resampling = rsmp("holdout"),
  measure = msr("classif.ce"),
  terminator = trm("evals", n_evals = 20),
  fselector = fs("random_search")
)

fselected_rpart
```

And of course it should be worth it to compare our variable-selected learner with a learner that uses all variables, just to make sure we're not wasting our time:

```{r fselect-bm}
design <- benchmark_grid(
  tasks = spam_task,
  learners = list(
    fselected_rpart,
    lrn("classif.rpart", predict_type = "prob")
  ),
  resamplings = rsmp("cv", folds = 3)
)

bmr <- benchmark(design)
bmr$aggregate(msr("classif.auc"))
```

Of course this is essentially another form of tuning, and doings feature selection with untuned learners is not going to give you the best possible performance in each iteration, but it gives you a good set of features to start your actual hyperparameter tuning with.


## Your turn! (For some other time)

- Try out the bike sharing task (`tsk("bike_sharing")`)
- [Read the docs](https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset) to see the meaning of each feature
- Try out different feature selection approaches!

Note that this task has a few more observations, so it's going to take a bit longer.  
We don't want to spend the in-person session staring overheating laptops, so you can try this out in your own time!
