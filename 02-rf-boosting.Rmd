---
title: "02: Random Forest & Boosting"
date: "`r Sys.time()`"
output: 
  html_notebook: 
    toc: yes
    theme: flatly
    number_sections: yes
---

```{r setup}
library(mlr3verse) # Loads all the mlr3 stuff
library(ggplot2)   # For plotting

# Just telling mlr3 to be quiet unless something broke
lgr::get_logger("mlr3")$set_threshold("error")
```

# Task Setup

As we've seen last time, our penguin data is pretty easy for our learners.
We need something a little more complex, meaning more observations (n)
and a couple more predictors (p).  
If you're looking for ready-made example tasks for your own experimentation,
mlr3 comes with a couple you can try. The procedure is similar to `mlr_learners`
and `mlr_measures`, but this time it's, you guessed it,  

```{r task-dictionary}
mlr_tasks

# As a neat little table with all the relevant info
as.data.table(mlr_tasks)
```

I suggest we try a two-class (i.e. binary) classification problem next, and
maybe something we can probably all somewhat relate to: Spam detection.

```{r task-spam}
spam_task <- tsk("spam")

# Outcome categories
spam_task$class_names
```

## Your turn!

Explore the task a little (you can use its built-in methods `spam_task$...`)
You're given a new analysis problem, what are the important properties here?

## Train & Test split

```{r}
set.seed(26)
spam_train <- sample(spam_task$nrow, 2/3 * spam_task$nrow)
spam_test <- setdiff(seq_len(spam_task$nrow), spam_train)
```

# Resampling

There's [lots of resampling strategies](https://mlr3book.mlr-org.com/resampling.html), but you usually can't go too wrong with cross validation (CV), which should not take too much time and computing power on small datasets like ours.

Instead of training our model just once, we're going to train it 5 times, which should give us a better idea of the accuracy we can expect:

```{r knn-resample-cv}
resampcv5 <- rsmp("cv", folds = 5)
rr <- resample(
  task = spam_task, 
  learner = lrn("classif.kknn", k = 9),
  resampling = resampcv5
)
```

Now we have a look at our predictions again, per resampling iteration:

```{r knn-resample-acc}
measure_auc <- msr("classif.auc")
rr$score(measure_auc)[, .(iteration, classif.auc)]
```

And on average over the resampling iterations:

```{r knn-resample-acc-avg}
rr$aggregate(measure_acc)

autoplot(rr, measure = measure_acc)

autoplot(rr$prediction(), type = "roc")
```


Note how we got to train our learner, do cross validate and get scores
all without having to do any extra work.
That's why we use mlr3 instead of doing everything manually - abstraction is nice.

## Comparing against a baseline

```{r}
learners <- list(
  lrn("classif.kknn", id = "knn", predict_type = "prob"),
  lrn("classif.rpart", id = "tree", predict_type = "prob"),
  lrn("classif.featureless", id = "Baseline", predict_type = "prob")
) 

design <- benchmark_grid(
  tasks = spam_task,       # Still the same task
  learners = learners,     # The new list of learners
  resamplings = resampcv5  # Same resampling strategy as before
) 

# Run the benchmark and save the results
bmr <- benchmark(design)
```

`bmr` contains all we'd like to know about out comparison, let's take a look:

```{r}
# Extract per-iteration accuracy per learner
bmr$score(measure_auc)[1:5, .(learner_id, iteration, classif.auc)]
```

And if we want to see what worked better overall:

```{r}
bmr$aggregate(measure_auc)[, .(learner_id, classif.auc)]

autoplot(bmr, measure = measure_auc)
```

# Random Forests & Boosting

Armed with our new model comparison skills, we can add Random Forests and
Gradient Boosting to the mix!

```{r}
lrn_ranger <- lrn("classif.ranger", predict_type = "prob")

lrn_ranger$train(spam_task, row_ids = spam_train)

spam_pred_ranger <- lrn_ranger$predict(spam_task, row_ids = spam_test)

spam_pred_ranger$score(msr("classif.acc"))
autoplot(spam_pred_ranger, type = "roc")
```

```{r}
lrn_xgboost <- lrn("classif.xgboost", predict_type = "prob")

lrn_xgboost$train(spam_task, row_ids = spam_train)

spam_pred_xgboost <- lrn_xgboost$predict(spam_task, row_ids = spam_test)

spam_pred_xgboost$score(msr("classif.acc"))
```


```{r}
autoplot(spam_pred_xgboost, type = "roc")

spam_pred_xgboost$score(msr("classif.auc"))
```

## Comparing all methods

```{r}
learners <- list(
  lrn("classif.kknn", id = "knn", predict_type = "prob"),
  lrn("classif.rpart", id = "tree", predict_type = "prob"),
  lrn("classif.ranger", id = "forest", predict_type = "prob"),
  lrn("classif.xgboost", id = "boost", predict_type = "prob")
) 

design <- benchmark_grid(
  tasks = spam_task,       # Still the same task
  learners = learners,     # The new list of learners
  resamplings = resampcv5  # Same resampling strategy as before
) 

# Run the benchmark and save the results
bmr <- benchmark(design)
```

```{r}
autoplot(bmr, measure = measure_auc)
```

