---
title: "04: Feature Selection & Importance"
date: "`r Sys.time()`"
output:
  html_notebook:
  toc: yes
theme: flatly
number_sections: yes
editor_options: 
  chunk_output_type: console
---

```{r setup}
library(mlr3verse)  # All the mlr3 things
library(ggplot2)    # For plotting
library(iml)        # Interpretability methods

# Spam Task setup
spam_task <- tsk("spam")
set.seed(26)
spam_train <- sample(spam_task$nrow, 2/3 * spam_task$nrow)
spam_test <- setdiff(seq_len(spam_task$nrow), spam_train)

# Penguin Task setup
penguins <- na.omit(palmerpenguins::penguins)
penguin_task <- TaskClassif$new(
  id = "penguins", 
  backend = penguins, 
  target = "species"
)
# penguin_task$col_roles$feature <- c("bill_depth_mm", "bill_length_mm", "body_mass_g", "flipper_length_mm")
set.seed(26)
penguin_train <- sample(penguin_task$nrow, 2/3 * penguin_task$nrow)
penguin_test <- setdiff(seq_len(penguin_task$nrow), penguin_train)
```



# Feature Selection

See also: 

- [mlr3gallery post with example](https://mlr3gallery.mlr-org.com/posts/2020-09-14-mlr3fselect-basic/)
- [mlr3book](https://mlr3book.mlr-org.com/optimization.html#fs)

Selecting features with `{mlr3}` is similar to parameter tuning: We need to set
a budget (e.g. 20 evaluations like before) and a criterion (like the auc.)
with a resampling strategy (here holdout for simplicity).

```{r}
fselect_instance = FSelectInstanceSingleCrit$new(
  task = spam_task,
  learner = lrn("classif.rpart", predict_type = "prob"),
  resampling = rsmp("holdout"),
  measure = msr("classif.auc"),
  terminator = trm("evals", n_evals = 20)
)

fselect_instance
```

There are multiple feature selection methods available:
- Random Search (`FSelectorRandomSearch`)
- Exhaustive Search (`FSelectorExhaustiveSearch`)
- Sequential Search (`FSelectorSequential`)
- Recursive Feature Elimination (`FSelectorRFE`)
- Design Points (`FSelectorDesignPoints`)

```{r}
mlr_fselectors
```

As you might be able to imagine, doing an exhaustive search is not often feasible when we're working with a lot of features. For a dataset with 10
features, examining every possible subset of features would yield over 50 possible combinations, and for our spam dataset with over 50 features we're well past 1500 combinations.
Random search it is, then!

```{r}
fselector <- fs("random_search")

fselector$optimize(fselect_instance)
```

```{r}
fselect_instance$result_feature_set

fselect_instance$result_y
```

```{r}
as.data.table(fselect_instance$archive)[1:5, ]
```

Similar to the `AutoTuner` we used for parameter tuning, there's also
an `AutoFSelector` which basically works the same way, giving us an optimized
learner as a result

```{r}
fselected_rpart <- AutoFSelector$new(
  learner = lrn("classif.rpart", predict_type = "prob"),
  resampling = rsmp("holdout"),
  measure = msr("classif.ce"),
  terminator = trm("evals", n_evals = 20),
  fselector = fs("random_search")
)

fselected_rpart
```

And of course it should be worth it to compare our variable-selected learner
with a learner that uses all variables, just to make sure we're not sacrificing any predictive performance:

```{r}
bmgrid <- benchmark_grid(
  task = spam_task,
  learner = list(
    fselected_rpart, 
    lrn("classif.rpart", predict_type = "prob")
  ),
  resampling = rsmp("cv", folds = 3)
)

bmr <- benchmark(bmgrid)
bmr$aggregate(msr("classif.auc"))
```

# Feature Importance

Interpretability is a huge topic, and there's a lot to explore - we'll cover some basics and if you're interested, you can always finde more at

- The [`{iml}` package](https://christophm.github.io/iml/articles/intro.html)
- The [Interpretable Machine Learning book](https://christophm.github.io/interpretable-ml-book)
- The [`{mlr3}` book](https://mlr3book.mlr-org.com/interpretation.html)

Before we get started with the general methods, it should be noted that some learners bring their own method-specific importance measures.
Random Forests (via `{ranger}`) for example has some built-in importance
metrics, like the corrected Gini impurity:

```{r ranger-importance}
lrn_ranger <- lrn("classif.ranger", importance = "impurity_corrected")
lrn_ranger$train(penguin_task)

sort(lrn_ranger$importance(), decreasing = TRUE)
```

Which shows us that for our penguins, `bill_length_mm` is probably the most relevant feature, whereas `body_mass_g` does not turn out to be as important
with regard to species classification.


## Feature Importance with `{iml}`

We can to the same (or a similar) thing with a general approach provided by the `{iml}` package, which lets us analyze any given learner wrapped by `{mlr3}` (and many other models created outside `mlr3`).

It has a similar object-oriented approach as `{mlr3}` with a `Predictor` object
we create using our learner and our data, separately for predictors and target:

```{r iml-predictor}
lrn_ranger <- lrn("classif.ranger", predict_type = "prob")
lrn_ranger$train(penguin_task)

penguin_x <- penguins[names(penguins) != "species"]
predictor_rf <- Predictor$new(lrn_ranger, data = penguin_x, y = penguins$species)
```

We can then create a `FeatureImp` object for our importance values:

```{r iml-importance}
penguin_importance <- FeatureImp$new(predictor_rf, loss = "ce")

plot(penguin_importance)
```

This type of feature importance is based on permutation, i.e. how much the prediction
error changes after a feature was randomly shuffled. 
It's not the same approach as the one we saw with ranger, but it's model-agnostic,
meaning it can be applied to any method.

In some cases, different methods may disagree.  
As an example, let's look at the Boston Housing dataset.  
This dataset contains, well, housing data for various towns which can be used to predict the 
median value of homes (variable `medv`, see also `?MASS::Boston`).  
Since this is a regression problem, we switch to `regr.ranger` as our learner:

```{r boston-omportance-example}
boston_task <- tsk("boston_housing")

# Ranger: Gini impurity
lrn_ranger <- lrn("regr.ranger", importance = "impurity_corrected")

lrn_ranger$train(boston_task)

sort(lrn_ranger$importance(), decreasing = TRUE)

# iml: Permutation feature importance
boston_predictor <- Predictor$new(
  model = lrn("regr.ranger")$train(boston_task), 
  data = boston_task$data(cols = boston_task$feature_names), 
  y = boston_task$data(cols = "medv"))

boston_importance <- FeatureImp$new(boston_predictor, loss = "mse")

# Merging importance results
# Also rescaling importance measures for visual comparison
importance_ranger <- data.frame(
  feature = names(lrn_ranger$importance()),
  importance = lrn_ranger$importance(),
  method = "Gini",
  row.names = NULL
)
importance_ranger$importance <- importance_ranger$importance/max(importance_ranger$importance)

importance_iml <- boston_importance$results[c("feature", "importance")]
importance_iml$importance <- importance_iml$importance/max(importance_iml$importance)
importance_iml$method <- "Permutation"

# Binding them together & plotting
importance_df <- rbind(importance_ranger, importance_iml)

ggplot(importance_df, aes(y = feature, x = importance, fill = method)) +
  geom_col(position = "dodge") +
  labs(
    title = "Feature importance comparison",
    subtitle = "Boston Housing regression task w/ Random Forest",
    x = "Importance (Rescaled within method)",
    y = "Feature", fill = "Importance Method"
  ) +
  theme_minimal() +
  theme(legend.position = "top")
```

## Your Turn!

So far we've only looked at feature importance based on a Random Forest learner,
but it's also possible that importance scores vary when different learners are applied.

Based on the `{iml}` code above, train two or more different learners on a task
of your choice (`tsk("boston")`, `tsk("spam")`, ...) and calculate feature importances
for both learners.

Do they agree?

## Example Code

```{r example-featureimp-comparison}
# Boston housing, but only numeric features due do SVM limitation
boston_task$col_roles$feature <- c("age", "b", "crim", "dis", "indus", "lat", "lon", 
"lstat", "nox", "ptratio", "rad", "rm", "tax", "tract", "zn")

# A bag of predictors
predictor_rf <- Predictor$new(
  model = lrn("regr.ranger")$train(boston_task), 
  data = boston_task$data(cols = boston_task$feature_names), 
  y = boston_task$data(cols = "medv"))

predictor_svm <- Predictor$new(
  model = lrn("regr.svm")$train(boston_task), 
  data = boston_task$data(cols = boston_task$feature_names), 
  y = boston_task$data(cols = "medv"))

predictor_lm <- Predictor$new(
  model = lrn("regr.lm")$train(boston_task), 
  data = boston_task$data(cols = boston_task$feature_names), 
  y = boston_task$data(cols = "medv"))

predictor_xgboost <- Predictor$new(
  model = lrn("regr.xgboost")$train(boston_task), 
  data = boston_task$data(cols = boston_task$feature_names), 
  y = boston_task$data(cols = "medv"))

# An assortment of importances
importance_rf <- FeatureImp$new(predictor_rf, loss = "mse")
importance_svm <- FeatureImp$new(predictor_svm, loss = "mse")
importance_lm <- FeatureImp$new(predictor_lm, loss = "mse")
importance_xgboost <- FeatureImp$new(predictor_xgboost, loss = "mse")

# An entanglement of plots
plot(importance_rf)
plot(importance_svm)
plot(importance_lm)
plot(importance_xgboost)
```

