---
title: "SVMs and more tuning"
editor_options: 
  chunk_output_type: console
---

```{r setup}
#| message: false
library(mlr3verse) # All the mlr3 things
library(ggplot2) # For plotting
# Silence output during tuning, mostly for cleaner output on the website
lgr::get_logger("mlr3")$set_threshold("error")

# Spam Task setup
spam_task <- tsk("spam")
set.seed(26)

# train/test split
spam_split <- partition(spam_task, ratio = 2 / 3)
```

Goals of this part:

1.  Introduce SVMs
2.  Tune an SVM with a more complex setup

# Support Vector Machines

Let's circle back to new learners and explore SVMs a little by trying out different kernels at the example of our penguin dataset we used in the beginning:

```{r penguins}
penguins <- na.omit(palmerpenguins::penguins)

ggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +
  geom_point() +
  labs(
    title = "Palmer Penguins",
    x = "Flipper Length [mm]",
    y = "Body Mass [g]",
    color = "Species"
  ) +
  theme_minimal()
```

Since we don't care about prediction accuracy for now, we'll use the whole dataset for training and prediction. Please only do this with toy data ðŸ™ƒ.

For the SVM algorithm itself, we use the `svm` learner from the `{e1071}` package (great name, I know) but once again use `{mlr3}`'s interface

According to the docs (`?e1071::svm`) we have the choice of the following kernels:

-   `"linear"`: $u'v$
-   `"polynomial"`: $(\mathtt{gamma} \cdot u' \cdot v + \mathtt{coef0})^\mathtt{degree}$
-   `"radial"`: $\exp(-\mathtt{gamma} \cdot |u-v|^2)$
-   `"sigmoid"`: $\tanh(\mathtt{gamma} \cdot u'v + \mathtt{coef0})$

Where `gamma`, `degree`, and `coef0` are further hyperparameters.

```{r svm-learner-default}
svm_learner <- lrn("classif.svm")

# What parameters do we have and what's the default kernel?
svm_learner$param_set
```

## Your Turn!

Below you have a boilerplate for

a)  Creating an SVM learner and train it on the penguin dataset with 2 predictors
b)  Plotting decision boundaries with it (using the `{mlr3}` helper function)

Run the code below once to see what linear decision boundaries look like, then pick different kernels from the list above and run it again.

-   What kernel would you pick just by the looks of the boundaries?
-   How do the boundaries change if you also adjust the other hyperparameters?
-   Try picking any other two variables as features (`penguin_task$col_info`)

```{r penguin-task-2-predictors}
penguin_task <- as_task_classif(
  na.omit(palmerpenguins::penguins),
  target = "species"
)
penguin_task$col_roles$feature <- c("body_mass_g", "flipper_length_mm")
```

```{r svm-decision-boundaries}
# Create the learner, picking a kernel and/or other hyperparams
svm_learner <- lrn("classif.svm", kernel = "polynomial", degree = 3)

# Plot decision boundaries
plot_learner_prediction(
  learner = svm_learner,
  task = penguin_task
)
```

```{r svm-kernels-your-turn}
# your code
```

{{< include solutions/05-1-svm-kernels.qmd >}}

## SVM-Tuning

Let's try a more complex tuning experiment, based on the spam task from before.

We'll create a new SVM learner object and this time explicitly tell it which classification to do --- that's the default value anyway, but `{mlr3}` wants us to be explicit here for tuning:

```{r svm-learner}
svm_learner <- lrn(
  "classif.svm",
  predict_type = "prob",
  type = "C-classification"
)
```

First up we'll define our search space, meaning the range of parameters we want to test out. Since `kernel` is a categorical parameter (i.e. no numbers, just names of kernels), we'll define the search space for that parameter by just passing the names of the kernels to the `p_fct()` helper function that defines `factor`-parameters in `{mlr3}`.

The interesting thing here is that some parameters are only relevant for some kernels, which we can declare via a `depends` argument:

```{r svm-search-space-short}
search_space_svm = ps(
  kernel = p_fct(c("linear", "polynomial", "radial", "sigmoid")),
  # Degree is only relevant if "kernel" is "polynomial"
  degree = p_int(lower = 1, upper = 7, depends = kernel == "polynomial")
)
```

We can create an example design grid to inspect our setup and see that `degree` is `NA` for cases where `kernel` is not `"polynomial"`, just as we expected

```{r svm-search-space-inspect}
generate_design_grid(search_space_svm, resolution = 3)
```

## Your Turn!

The above should get you started to...

1.  Create a `search_space_svm` like above, tuning...

-   `cost` from 0.1 to 1 (hint: `logscale = TRUE` or e.g. `trafo = function(x) 10^x`)
-   `kernel`, (like above example)
-   `degree`, as above, **only if** `kernel == "polynomial"`
-   `gamma`, from e.g. 0.01 to 0.2, **only if** `kernel` is polynomial, radial, sigmoid (hint: you can't use `kernel != "linear"` unfortunately, but `kernel %in% c(...)`) works

2.  Use the `auto_tuner` function as previously seen with

-   `svm_learner` (see above)
-   A resampling strategy (use `"holdout"` if runtime is an issue)
-   A measure (e.g. `classif.acc` or `classif.auc`)
-   The search space you created in 1.
-   A termination criterion (e.g. 40 evaluations)
-   Random search as your tuning strategy

3.  Train the auto-tuned learner and evaluate on the test set

What parameter settings worked best?

```{r svm-tune-your-turn}
# your code
```

{{< include solutions/05-2-svm-tune.qmd >}}

::: {.callout-tip title="SVM with categorical features"}
We have not covered this so far, but if you want to tran an SVM (or many other learners) on tasks with categorical ("nominal") features (usually `factor` in R), we first need to encode them in a numeric format. The simplest way is to perform dummy- or one-hot encoding, and mlr3 has a whole slew of these sorts of rpeprocessing capabilities.

This works by creating a *pipeline* using the `%>>%` oeprator (not to be confused with the magrittr-pipe `%>%` you might be familiar with!). We take the `encode` pipeline operation (`PipeOp`) and stack it on top of our learner we create as usual. At the end we convert it to a regular learner, and `lrn_svm` is now a regular `mlr3` learner we can use like any other, but with built-in encoding!

```{r svm-encode-demo}
lrn_svm_base <- lrn("classif.svm", predict_type = "prob")

lrn_svm <- po("encode") %>>%
  po("learner", lrn_svm_base) |>
  as_learner()

# Penguin task (including categoricals!)
penguin_task <- as_task_classif(
  na.omit(palmerpenguins::penguins),
  target = "species"
)

# Quick demo
lrn_svm$train(penguin_task)
lrn_svm$predict(penguin_task)
```

Pipelines are extremely useful and part of any reasonably complex machine learning pipeline, and to learn more you can read the [chapter in the mlr3book](https://mlr3book.mlr-org.com/chapters/chapter7/sequential_pipelines.html).

Note that PipeOps can have their own parameters, which could also be subject to tuning!
:::
