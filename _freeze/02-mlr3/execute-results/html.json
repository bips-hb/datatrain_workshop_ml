{
  "hash": "0169bd3caf6c74305d6d7c1db0b9308a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Introducing `{mlr3}`'\neditor_options:\n  chunk_output_type: console\n---\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggplot2) # For plotting\nlibrary(palmerpenguins) # For penguins\nlibrary(mlr3verse) # includes mlr3, mlr3learners, mlr3tuning, mlr3viz, ...\n```\n:::\n\n\nGoals of this part:\n\n1.  Introduce `{mlr3}` and do everythign we did before again, but nicer\n\n\n# Switching to `{mlr3}`\n\nNow, imagine you want to try out some more hyperparameters for either `rpart()` or `kknn()` or both, and then you want to compare the two --- that would probably be kind of tedious unless you write some wrapper functions, right?\nWell, luckily we're not the first people to do some machine learning!\n\nOur code in the previous section works (hopefully), but for any given model or algorithm, there's different R packages with slightly different interfaces, and memorizing or looking up how they work can be a tedious and error-prone task, especially when we want to repeat the same general steps with each learner.\n`{mlr3}` and add-on packages unify all the common tasks with a consistent interface.\n\n## Creating a task\n\nThe task encapsulates our **data**, including which **features** we're using for learning and wich variable we use as the **target** for prediction.\nTasks can be created in [multiple ways and some standard example tasks are available in mlr3](https://mlr3book.mlr-org.com/chapters/chapter2/data_and_basic_modeling.html), but we're taking the long way around.\n\nQuestions a task object answers:\n\n- What is the **task type**, what kind of prediction are we doing?\n    - Here: Classification (instead of e.g. regression)\n\n- What are we predicting on?\n    - The dataset (\"backend\"), could be a `data.frame`, `matrix`, or a proper data base\n\n- What variable are we trying to predict?\n    - The `target` variable, here `species`\n\n- Which variables are we using for predictions?\n    - The `feature` variables, which we can adjust\n    \nSo let's put our penguins into a task object for `{mlr3}` and inspect it:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Creating a classification task from our penguin data\npenguin_task <- as_task_classif(\n  na.omit(palmerpenguins::penguins),\n  id = \"penguins\",\n  target = \"species\"\n)\n\npenguin_task\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> ── <TaskClassif> (333x8) ───────────────────────────────────────────────────────\n#> • Target: species\n#> • Target classes: Adelie (44%), Gentoo (36%), Chinstrap (20%)\n#> • Properties: multiclass\n#> • Features (7):\n#>   • int (3): body_mass_g, flipper_length_mm, year\n#>   • dbl (2): bill_depth_mm, bill_length_mm\n#>   • fct (2): island, sex\n```\n\n\n:::\n:::\n\n\nWe can poke at it a little. Try typing `penguin_task$` and hit the Tab key to trigger completions.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Contains our penguin dataset\npenguin_task$data()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>        species bill_depth_mm bill_length_mm body_mass_g flipper_length_mm\n#>         <fctr>         <num>          <num>       <int>             <int>\n#>   1:    Adelie          18.7           39.1        3750               181\n#>   2:    Adelie          17.4           39.5        3800               186\n#>   3:    Adelie          18.0           40.3        3250               195\n#>   4:    Adelie          19.3           36.7        3450               193\n#>   5:    Adelie          20.6           39.3        3650               190\n#>  ---                                                                     \n#> 329: Chinstrap          19.8           55.8        4000               207\n#> 330: Chinstrap          18.1           43.5        3400               202\n#> 331: Chinstrap          18.2           49.6        3775               193\n#> 332: Chinstrap          19.0           50.8        4100               210\n#> 333: Chinstrap          18.7           50.2        3775               198\n#>         island    sex  year\n#>         <fctr> <fctr> <int>\n#>   1: Torgersen   male  2007\n#>   2: Torgersen female  2007\n#>   3: Torgersen female  2007\n#>   4: Torgersen female  2007\n#>   5: Torgersen   male  2007\n#>  ---                       \n#> 329:     Dream   male  2009\n#> 330:     Dream female  2009\n#> 331:     Dream   male  2009\n#> 332:     Dream   male  2009\n#> 333:     Dream female  2009\n```\n\n\n:::\n\n```{.r .cell-code}\n# We can ask it about e.g. our sample size and number of features\npenguin_task$nrow\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 333\n```\n\n\n:::\n\n```{.r .cell-code}\npenguin_task$ncol\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 8\n```\n\n\n:::\n\n```{.r .cell-code}\n# And what the classes are\npenguin_task$class_names\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] \"Adelie\"    \"Chinstrap\" \"Gentoo\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# What types of features do we have?\n# (relevant for learner support, some don't handle factors for example!)\npenguin_task$feature_types\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Key: <id>\n#>                   id    type\n#>               <char>  <char>\n#> 1:     bill_depth_mm numeric\n#> 2:    bill_length_mm numeric\n#> 3:       body_mass_g integer\n#> 4: flipper_length_mm integer\n#> 5:            island  factor\n#> 6:               sex  factor\n#> 7:              year integer\n```\n\n\n:::\n:::\n\n\nWe can further inspect and modify the task after the fact if we choose:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Display feature and target variable assignment\npenguin_task$col_roles[c(\"feature\", \"target\")]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> $feature\n#> [1] \"bill_depth_mm\"     \"bill_length_mm\"    \"body_mass_g\"      \n#> [4] \"flipper_length_mm\" \"island\"            \"sex\"              \n#> [7] \"year\"             \n#> \n#> $target\n#> [1] \"species\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Maybe not all variables are useful for this task, let's remove some\npenguin_task$set_col_roles(\n  cols = c(\"island\", \"sex\", \"year\"),\n  remove_from = \"feature\"\n)\n\n# We can also explicitly assign the feature columns\npenguin_task$col_roles$feature <- c(\"body_mass_g\", \"flipper_length_mm\")\n\n# Check what our current variables and roles are\npenguin_task$col_roles[c(\"feature\", \"target\")]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> $feature\n#> [1] \"body_mass_g\"       \"flipper_length_mm\"\n#> \n#> $target\n#> [1] \"species\"\n```\n\n\n:::\n:::\n\n\nSome variables may have missing values --- if we had not excluded them in the beginning, you would find them here:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npenguin_task$missings()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>           species       body_mass_g flipper_length_mm \n#>                 0                 0                 0\n```\n\n\n:::\n:::\n\n\n### Train and test split\n\nWe're mixing things up with a new train/test split, just for completeness in the example.\nFor `{mlr3}`, we only need to save the indices / row IDs.\nThere's a handy `partition()` function that does the same think we did with `sample()` earlier, so let's use that!\nWe create another 2/3 train-test split. 2/3 is actually the default in `partition()` anyway.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(26)\npenguin_split <- partition(penguin_task, ratio = 2 / 3)\n\n# Contains vector of row_ids for train/test set\nstr(penguin_split)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> List of 3\n#>  $ train     : int [1:222] 2 3 4 8 9 10 13 17 19 20 ...\n#>  $ test      : int [1:111] 1 5 6 7 11 12 14 15 16 18 ...\n#>  $ validation: int(0)\n```\n\n\n:::\n:::\n\n\nWe can now use `penguin_split$train` and `penguin_split$test` with every `mlr3` function that has a `row_ids` argument.\n\n\n::: {.callout-caution}\nThe `row_ids` of a task are not necessarily `1:N` --- there is no guarantee they start at 1, go up to `N`, or contain all integers in between. We can generally only expect them to be unique within a task!\n:::\n\n## Picking a Learner\n\nA learner encapsulates the fitting algorithm as well as any relevant hyperparameters, and `{mlr3}` supports a whole lot of learners to choose from.\nWe'll keep using `{kknn}` and `{rpart}` in the background for the classification task, but we'll use `{mlr3}` on top of them for a consistent interface.\nSo first, we have to find the learners we're looking for.\n\nThere's a lot more [about learners in the mlr3 book](https://mlr3book.mlr-org.com/chapters/chapter2/data_and_basic_modeling.html#sec-learners), \nbut for now we're happy with the basics.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Lots of learners to choose from here:\nmlr_learners\n# Or as a large table:\nas.data.table(mlr_learners)\n\n# To show all classification learners (in *currently loaded* packages!)\nmlr_learners$keys(pattern = \"classif\")\n\n# There's also regression learner we don't need right now:\nmlr_learners$keys(pattern = \"regr\")\n\n# For the kknn OR rpart learners, we can use regex\nmlr_learners$keys(pattern = \"classif\\\\.(kknn|rpart)\")\n```\n:::\n\n\nNow that we've identified our learners, we can get it quickly via the `lrn()` helper function:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nknn_learner <- lrn(\"classif.kknn\")\n```\n:::\n\n\nMost things in `mlr3` have a `$help()` method which opens the R help page:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nknn_learner$help()\n```\n:::\n\n\nWhat parameters does this learner have?\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nknn_learner$param_set\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> <ParamSet(6)>\n#>             id    class lower upper nlevels default  value\n#>         <char>   <char> <num> <num>   <num>  <list> <list>\n#> 1:           k ParamInt     1   Inf     Inf       7      7\n#> 2:    distance ParamDbl     0   Inf     Inf       2 [NULL]\n#> 3:      kernel ParamFct    NA    NA      10 optimal [NULL]\n#> 4:       scale ParamLgl    NA    NA       2    TRUE [NULL]\n#> 5:     ykernel ParamUty    NA    NA     Inf  [NULL] [NULL]\n#> 6: store_model ParamLgl    NA    NA       2   FALSE [NULL]\n```\n\n\n:::\n:::\n\n\nWe can get them by name as well:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nknn_learner$param_set$ids()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] \"k\"           \"distance\"    \"kernel\"      \"scale\"       \"ykernel\"    \n#> [6] \"store_model\"\n```\n\n\n:::\n:::\n\n\nSetting parameters leaves the others as default:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nknn_learner$configure(k = 5)\n\nknn_learner$param_set\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> <ParamSet(6)>\n#>             id    class lower upper nlevels default  value\n#>         <char>   <char> <num> <num>   <num>  <list> <list>\n#> 1:           k ParamInt     1   Inf     Inf       7      5\n#> 2:    distance ParamDbl     0   Inf     Inf       2 [NULL]\n#> 3:      kernel ParamFct    NA    NA      10 optimal [NULL]\n#> 4:       scale ParamLgl    NA    NA       2    TRUE [NULL]\n#> 5:     ykernel ParamUty    NA    NA     Inf  [NULL] [NULL]\n#> 6: store_model ParamLgl    NA    NA       2   FALSE [NULL]\n```\n\n\n:::\n:::\n\n\nIdentical methods to set multiple or a single hyperparam, just in case you see them in other code somewhere:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nknn_learner$param_set$set_values(k = 9)\nknn_learner$param_set$values <- list(k = 9)\nknn_learner$param_set$values$k <- 9\n```\n:::\n\n\n\nIn practice, we usually set the parameters directly when we construct the learner object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nknn_learner <- lrn(\"classif.kknn\", k = 7)\nknn_learner$param_set\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> <ParamSet(6)>\n#>             id    class lower upper nlevels default  value\n#>         <char>   <char> <num> <num>   <num>  <list> <list>\n#> 1:           k ParamInt     1   Inf     Inf       7      7\n#> 2:    distance ParamDbl     0   Inf     Inf       2 [NULL]\n#> 3:      kernel ParamFct    NA    NA      10 optimal [NULL]\n#> 4:       scale ParamLgl    NA    NA       2    TRUE [NULL]\n#> 5:     ykernel ParamUty    NA    NA     Inf  [NULL] [NULL]\n#> 6: store_model ParamLgl    NA    NA       2   FALSE [NULL]\n```\n\n\n:::\n:::\n\n\nWe'll save the `{rpart}` learner for later, but all the methods are the same because they are all `Learner` objects.\n\n## Training and evaluating\n\nWe can train the learner with default parameters once to see if it works as we expect it to.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Train learner on training data\nknn_learner$train(penguin_task, row_ids = penguin_split$train)\n\n# Look at stored model, which for knn is not very interesting\nknn_learner$model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> $formula\n#> species ~ .\n#> NULL\n#> \n#> $data\n#>        species body_mass_g flipper_length_mm\n#>         <fctr>       <int>             <int>\n#>   1:    Adelie        3800               186\n#>   2:    Adelie        3250               195\n#>   3:    Adelie        3450               193\n#>   4:    Adelie        3200               182\n#>   5:    Adelie        3800               191\n#>  ---                                        \n#> 218: Chinstrap        3650               189\n#> 219: Chinstrap        3650               195\n#> 220: Chinstrap        3400               202\n#> 221: Chinstrap        3775               193\n#> 222: Chinstrap        3775               198\n#> \n#> $pv\n#> $pv$k\n#> [1] 7\n#> \n#> \n#> $kknn\n#> NULL\n```\n\n\n:::\n:::\n\n\nAnd we can make predictions on the test data:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nknn_prediction <- knn_learner$predict(\n  penguin_task,\n  row_ids = penguin_split$test\n)\nknn_prediction\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> ── <PredictionClassif> for 111 observations: ───────────────────────────────────\n#>  row_ids     truth  response\n#>        1    Adelie    Adelie\n#>        5    Adelie    Adelie\n#>        6    Adelie    Adelie\n#>      ---       ---       ---\n#>      326 Chinstrap Chinstrap\n#>      329 Chinstrap Chinstrap\n#>      332 Chinstrap    Gentoo\n```\n\n\n:::\n:::\n\n\nOur predictions are looking quite reasonable, mostly:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Confusion matrix we got via table() previously\nknn_prediction$confusion\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>            truth\n#> response    Adelie Chinstrap Gentoo\n#>   Adelie        48         9      0\n#>   Chinstrap      8         5      1\n#>   Gentoo         3         1     36\n```\n\n\n:::\n:::\n\n\nTo calculate the prediction accuracy, we don't have to do any math in small steps.\n`{mlr3}` comes with lots of measures (like accuracy) we can use, they're organized \nin the `mlr_measures` object (just like `mlr_learners`). \n\nWe're using `\"classif.acc\"` here with the shorthand function `msr()`, and \nscore our predictions with this measure, using the `$score()` method.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Available measures for classification tasks\nmlr_measures$keys(pattern = \"classif\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  [1] \"classif.acc\"         \"classif.auc\"         \"classif.bacc\"       \n#>  [4] \"classif.bbrier\"      \"classif.ce\"          \"classif.costs\"      \n#>  [7] \"classif.dor\"         \"classif.fbeta\"       \"classif.fdr\"        \n#> [10] \"classif.fn\"          \"classif.fnr\"         \"classif.fomr\"       \n#> [13] \"classif.fp\"          \"classif.fpr\"         \"classif.logloss\"    \n#> [16] \"classif.mauc_au1p\"   \"classif.mauc_au1u\"   \"classif.mauc_aunp\"  \n#> [19] \"classif.mauc_aunu\"   \"classif.mauc_mu\"     \"classif.mbrier\"     \n#> [22] \"classif.mcc\"         \"classif.npv\"         \"classif.ppv\"        \n#> [25] \"classif.prauc\"       \"classif.precision\"   \"classif.recall\"     \n#> [28] \"classif.sensitivity\" \"classif.specificity\" \"classif.tn\"         \n#> [31] \"classif.tnr\"         \"classif.tp\"          \"classif.tpr\"        \n#> [34] \"debug_classif\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Scores according to the selected measure\nknn_prediction$score(msr(\"classif.acc\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> classif.acc \n#>   0.8018018\n```\n\n\n:::\n\n```{.r .cell-code}\n# The inverse: Classification error (1 - accuracy)\nknn_prediction$score(msr(\"classif.ce\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> classif.ce \n#>  0.1981982\n```\n\n\n:::\n:::\n\n\nAs a bonus feature, `{mlr3}` also makes it easy for us to plot the decision boundaries for a two-predictor case, so we don't have to manually predict on a grid anymore.\n\n(You can ignore any warning messages from the plot here)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot_learner_prediction(\n  learner = knn_learner,\n  task = penguin_task\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> INFO  [08:35:29.797] [mlr3] Applying learner 'classif.kknn' on task 'penguins' (iter 1/1)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: Raster pixels are placed at uneven horizontal intervals and will be shifted\n#> ℹ Consider using `geom_tile()` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](02-mlr3_files/figure-html/mlr3-plot-decision-boundaries-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n## Your turn!\n\nNow that we've done the kNN fitting with `{mlr3}`, you can easily do the same thing with the `rpart`-learner!\nAll you have to do is switch the learner objects and give it a more fitting name.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# your code\n```\n:::\n\n\n\n::: {.callout-tip title=\"Example solution\" collapse =\"true\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Picking the learner\nrpart_learner <- lrn(\"classif.rpart\")\n\n# What parameters does this learner have?\nrpart_learner$param_set$ids()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  [1] \"cp\"             \"keep_model\"     \"maxcompete\"     \"maxdepth\"      \n#>  [5] \"maxsurrogate\"   \"minbucket\"      \"minsplit\"       \"surrogatestyle\"\n#>  [9] \"usesurrogate\"   \"xval\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Setting parameters (omit to use the defaults)\nrpart_learner$param_set$values$maxdepth <- 20\n\n# Train\nrpart_learner$train(penguin_task, row_ids = penguin_split$train)\n\n# Predict\nrpart_prediction <- rpart_learner$predict(penguin_task, row_ids = penguin_split$test)\n\nrpart_prediction$confusion\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>            truth\n#> response    Adelie Chinstrap Gentoo\n#>   Adelie        52         9      0\n#>   Chinstrap      5         3      0\n#>   Gentoo         2         3     37\n```\n\n\n:::\n\n```{.r .cell-code}\n# Accuracy\nrpart_prediction$score(msr(\"classif.acc\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> classif.acc \n#>   0.8288288\n```\n\n\n:::\n\n```{.r .cell-code}\nplot_learner_prediction(rpart_learner, penguin_task)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> INFO  [08:35:30.411] [mlr3] Applying learner 'classif.rpart' on task 'penguins' (iter 1/1)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: Raster pixels are placed at uneven horizontal intervals and will be shifted\n#> ℹ Consider using `geom_tile()` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](02-mlr3_files/figure-html/example-rpart-learner-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n:::\n\n\n# Useful links\n\n- [mlr3 cheatsheet with examples](https://cheatsheets.mlr-org.com/mlr3.pdf)\n- [mlr3 book](https://mlr3book.mlr-org.com/) \n",
    "supporting": [
      "02-mlr3_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}