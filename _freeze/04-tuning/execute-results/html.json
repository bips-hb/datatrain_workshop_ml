{
  "hash": "e4f226e7a7eabb64d72f628889f9d796",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Tuning\"\neditor_options: \n  chunk_output_type: console\n---\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse) # All the mlr3 things\nlibrary(ggplot2) # For plotting\nlgr::get_logger(\"mlr3\")$set_threshold(\"error\")\n\n# Spam Task setup\nspam_task <- tsk(\"spam\")\nset.seed(26)\n\n# train/test split\nspam_split <- partition(spam_task, ratio = 2 / 3)\n```\n:::\n\n\nGoals of this part:\n\n1.  Introduce hyperparameter tuning\n2.  Experiment with tuning different learners\n\n# Hyperparameter Tuning\n\nSo far we've seen four learners:\n\n1.  kNN via `{kknn}`\n2.  Decision Trees via `{rpart}`\n3.  Random Forest via `{ranger}`\n4.  Gradient Boosting via `{xgboost}`\n\nWe've gotten to know the first two a little, and now we'll also take a closer look at the second two.\n\nFirst we'll start doing some tuning with `{mlr3}` based on the **kNN** learner because it's nice and simple.\nWe saw that `k` is an important parameter, and it's an integer greater than 1 at least. To tune it, we also have to make a few other decisions, also using what we learned about (nested) resampling.\n\n-   What's our resampling strategy?\n-   What measure to we tune on?\n-   What does the parameter search space look like?\n-   How long to we tune? What's our *budget*?\n-   What's our tuning strategy?\n\nWe'll use `{mlr3}`'s [`auto_tuner`](https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html) for this because it's just so convenient:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Defining a search space: k is an integer, we look in range 3 to 51\nsearch_space_knn = ps(\n  k = p_int(lower = 3, upper = 51)\n)\n\ntuned_knn <- auto_tuner(\n  # The base learner we want to tune, optionally setting other parameters\n  learner = lrn(\"classif.kknn\", predict_type = \"prob\"),\n  # Resampling strategy for the tuning (inner resampling)\n  resampling = rsmp(\"cv\", folds = 3),\n  # Tuning measure: Maximize the classification accuracy\n  measure = msr(\"classif.bbrier\"),\n  # Setting the search space we defined above\n  search_space = search_space_knn,\n  # Budget: Try n_evals different values\n  terminator = trm(\"evals\", n_evals = 20),\n  # Strategy: Randomly try parameter values in the space\n  # (not ideal in this case but in general a good place to start)\n  tuner = tnr(\"random_search\")\n)\n\n# Take a look at the new tuning learner\ntuned_knn\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> ── <AutoTuner> (classif.kknn.tuned) ────────────────────────────────────────────\n#> • Model: -\n#> • Parameters: list()\n#> • Packages: mlr3, mlr3tuning, mlr3learners, and kknn\n#> • Predict Types: response and [prob]\n#> • Feature Types: logical, integer, numeric, factor, and ordered\n#> • Encapsulation: none (fallback: -)\n#> • Properties: multiclass and twoclass\n#> • Other settings: use_weights = 'error'\n#> • Search Space:\n#>        id    class lower upper nlevels\n#>    <char>   <char> <num> <num>   <num>\n#> 1:      k ParamInt     3    51      49\n```\n\n\n:::\n:::\n\n\nNow `tuned_knn` behaves the same way as any other Learner that has not been trained on any data yet --- first we have to train (and tune!) it on our spam training data. The result will be the best hyperparameter configuration of those we tried:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# If you're on your own machine with multiple cores available, you can parallelize:\nfuture::plan(\"multisession\", workers = 4)\n\n# Setting a seed so we get the same results -- train on train set only!\nset.seed(2398)\ntuned_knn$train(spam_task, row_ids = spam_split$train)\n```\n:::\n\n\n(Side task: Try the same but with the AUC measure --- do you get the same `k`?)\n\nWe can visualize the performance across all values of `k` we tried by accessing the tuning instance now included in the `tuned_knn` learner object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(tuned_knn$tuning_instance)\n```\n\n::: {.cell-output-display}\n![](04-tuning_files/figure-html/plot-knn-tuning-result-1.png){fig-align='center' width=672}\n:::\n\n```{.r .cell-code}\ntuned_knn$tuning_instance\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> ── <TuningInstanceBatchSingleCrit> ─────────────────────────────────────────────\n#> • State: Optimized\n#> • Objective: <ObjectiveTuningBatch>\n#> • Search Space:\n#>        id    class lower upper nlevels\n#>    <char>   <char> <num> <num>   <num>\n#> 1:      k ParamInt     3    51      49\n#> • Terminator: <TerminatorEvals> (n_evals=20, k=0)\n#> • Result:\n#>        k classif.bbrier\n#>    <int>          <num>\n#> 1:    14     0.07060497\n#> • Archive:\n#>     classif.bbrier     k\n#>              <num> <int>\n#>  1:           0.07    10\n#>  2:           0.08    43\n#>  3:           0.07     9\n#>  4:           0.08    48\n#>  5:           0.08    40\n#>  6:           0.07    17\n#>  7:           0.07    14\n#>  8:           0.07    26\n#>  9:           0.08    39\n#> 10:           0.07    26\n#> 11:           0.08    47\n#> 12:           0.07     7\n#> 13:           0.08     5\n#> 14:           0.08    48\n#> 15:           0.07    22\n#> 16:           0.07     6\n#> 17:           0.08    38\n#> 18:           0.08    49\n#> 19:           0.07    30\n#> 20:           0.07    26\n#>     classif.bbrier     k\n```\n\n\n:::\n:::\n\n\n(See also docs at `?mlr3viz:::autoplot.TuningInstanceSingleCrit`)\n\nAnd we can get the hyperparameter results that worked best in the end:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntuned_knn$tuning_result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>        k learner_param_vals  x_domain classif.bbrier\n#>    <int>             <list>    <list>          <num>\n#> 1:    14          <list[1]> <list[1]>     0.07060497\n```\n\n\n:::\n:::\n\n\nNow that we've tuned on the training set, it's time to evaluate on the test set:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntuned_knn_pred <- tuned_knn$predict(spam_task, row_ids = spam_split$test)\n\n# Accuracy and AUC\ntuned_knn_pred$score(msrs(c(\"classif.acc\", \"classif.auc\", \"classif.bbrier\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>    classif.acc    classif.auc classif.bbrier \n#>     0.90482399     0.96285949     0.07076836\n```\n\n\n:::\n:::\n\n\nThat was basically the manual way of doing nested resampling with an inner resampling strategy (CV) and an outer resampling strategy of holdout (the `spam_train` and `spam_test` sets).\n\nIn the next step we're going to compare the knn learner with the decision tree learner, and for that we need a proper nested resampling:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Set up the knn autotuner again\ntuned_knn <- auto_tuner(\n  # The base learner we want to tune, optionally setting other parameters\n  learner = lrn(\"classif.kknn\", predict_type = \"prob\"),\n  # Resampling strategy for the tuning (inner resampling)\n  resampling = rsmp(\"cv\", folds = 3),\n  # Tuning measure: Maximize the classification accuracy\n  measure = msr(\"classif.bbrier\"),\n  # Setting the search space we defined above\n  search_space = ps(k = p_int(lower = 3, upper = 51)),\n  # Budget: Try n_evals different values\n  terminator = trm(\"evals\", n_evals = 20),\n  # Strategy: Randomly try parameter values in the space\n  # (not ideal in this case but in general a good place to start)\n  tuner = tnr(\"random_search\")\n)\n\n# Set up resampling with the ready-to-be-tuned learner and outer resampling: CV\nknn_nested_tune <- resample(\n  task = spam_task,\n  learner = tuned_knn,\n  resampling = rsmp(\"cv\", folds = 3), # this is effectively the outer resampling\n  store_models = TRUE\n)\n\n# Extract inner tuning results, since we now tuned in multiple CV folds\n# Folds might conclude different optimal k\n# AUC is averaged over inner folds\nextract_inner_tuning_results(knn_nested_tune)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>    iteration     k classif.bbrier learner_param_vals  x_domain task_id\n#>        <int> <int>          <num>             <list>    <list>  <char>\n#> 1:         1    16     0.07293843          <list[1]> <list[1]>    spam\n#> 2:         2    16     0.07697195          <list[1]> <list[1]>    spam\n#> 3:         3    14     0.07406681          <list[1]> <list[1]>    spam\n#>            learner_id resampling_id\n#>                <char>        <char>\n#> 1: classif.kknn.tuned            cv\n#> 2: classif.kknn.tuned            cv\n#> 3: classif.kknn.tuned            cv\n```\n\n\n:::\n\n```{.r .cell-code}\n# Above combined individual results also accessible via e.g.\nknn_nested_tune$learners[[2]]$tuning_result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>        k learner_param_vals  x_domain classif.bbrier\n#>    <int>             <list>    <list>          <num>\n#> 1:    16          <list[1]> <list[1]>     0.07697195\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot of inner folds\nautoplot(knn_nested_tune$learners[[1]]$tuning_instance)\n```\n\n::: {.cell-output-display}\n![](04-tuning_files/figure-html/knn-nested-resampling-1.png){fig-align='center' width=672}\n:::\n\n```{.r .cell-code}\nautoplot(knn_nested_tune$learners[[2]]$tuning_instance)\n```\n\n::: {.cell-output-display}\n![](04-tuning_files/figure-html/knn-nested-resampling-2.png){fig-align='center' width=672}\n:::\n\n```{.r .cell-code}\nautoplot(knn_nested_tune$learners[[3]]$tuning_instance)\n```\n\n::: {.cell-output-display}\n![](04-tuning_files/figure-html/knn-nested-resampling-3.png){fig-align='center' width=672}\n:::\n\n```{.r .cell-code}\n# Get AUC and acc for outer folds\nknn_nested_tune$score(msrs(c(\"classif.acc\", \"classif.auc\")))[, .(\n  iteration,\n  classif.auc,\n  classif.acc\n)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>    iteration classif.auc classif.acc\n#>        <int>       <num>       <num>\n#> 1:         1   0.9637586   0.9087353\n#> 2:         2   0.9707207   0.9217731\n#> 3:         3   0.9613927   0.9034573\n```\n\n\n:::\n\n```{.r .cell-code}\n# Average over outer folds - our \"final result\" performance for kNN\n# AUC is averaged over outer folds\nknn_nested_tune$aggregate(msrs(c(\"classif.acc\", \"classif.auc\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> classif.acc classif.auc \n#>   0.9113219   0.9652906\n```\n\n\n:::\n:::\n\n\nSeems like a decent result?\nLet's try to beat it with some other learner!\n\n## Your Turn!\n\nAbove you have a boilerplate to tune your own learner.\nStart with either of the other three learners we've seen, pick one ore two hyperparameters to tune with a reasonable budget (note we have limited time and resources), tune on the training set and evaluate per AUC on the test set.\n\nSome pointers:\n\n-   Consult the Learner docs to see tuning-worthy parameters:\n\n    -   `lrn(\"classif.xgboost\")$help()` links to the `xgboost` help\n    -   `lrn(\"classif.rpart\")$help()` analogously for the decision tree\n    -   You can also see the documentation online, e.g. <https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.xgboost.html>\n\n-   Parameter search spaces in `ps()` have different types, see the help at `?paradox::Domain`\n    -   Use `p_int()` for integers, `p_dbl()` for real-valued params etc.\n\n-   If you don't know which parameter to tune, try the following:\n    -   `classif.xgboost`:\n        -   Important: `nrounds` (integer) (>= 1 (at least 50 or so))\n        -   Important: `eta` (double) (0 < eta < 1 (close to 0 probably))\n        -   Maybe: `max_depth` (integer)\n\n    -   `classif.rpart`:\n        -   `cp` (double)\n        -   Maybe: `maxdepth` (integer) (< 30)\n\n    -   `classif.ranger`:\n        -   `mtry` (integer) -> tune `mtry.ratio` (0 < `mtry.ratio` < 1)\n        -   `max.depth` (integer)\n\nNote: Instead of randomly picking parameters from the design space, we can also generate a grid of parameters and try those.\nWe'll not try that here for now, but you can read up on how to do that here: `?mlr_tuners_grid_search`.\n\n-> `generate_design_grid(search_space, resolution = 5)`\n\nAlso note that the cool thing about the `auto_tuner()` is that it behaves just like any other `mlr3` learner, but it automatically tunes itself.\nYou can plug it into `resample()` or `benchmark_grid()` just fine!\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# your code\n```\n:::\n\n\n\n::: {.callout-tip title=\"Example solution\" collapse =\"true\"}\n\n\nThe following example code uses the `holdout` resampling just to keep it fast --- when you have the time, using cross-validation (`\"cv\"`) will give you more reliable results.\n\n\nWe also use 6 threads for parallelization here, but you are free to adjust this according to your available hardware.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfuture::plan(\"multisession\", workers = 6)\n```\n:::\n\n\nThe tuning budget used here is just 50 evaluations, which as all you likely want to bump up a little if you have the time.\n\n::: {.panel-tabset}\n\n\n#### `rpart` Tuning\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Tuning setup\ntuned_rpart = auto_tuner(\n  learner = lrn(\"classif.rpart\", predict_type = \"prob\"),\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.auc\"),\n  search_space = ps(\n    cp = p_dbl(lower = 0.001, upper = 0.03),\n    maxdepth = p_int(lower = 1, upper = 30)\n  ),\n  terminator = trm(\"evals\", n_evals = 50),\n  tuner = tnr(\"random_search\")\n)\n\n# Tune!\ntuned_rpart$train(spam_task, row_ids = spam_split$train)\n\n# Evaluate!\ntuned_rpart$predict(spam_task, row_ids = spam_split$test)$score(msr(\n  \"classif.auc\"\n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> classif.auc \n#>   0.9215502\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check parameter results\nautoplot(tuned_rpart$tuning_instance)\n```\n\n::: {.cell-output-display}\n![](04-tuning_files/figure-html/example-tuned-rpart-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n#### `xgboost` Tuning\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Tuning setup\ntuned_xgboost = auto_tuner(\n  learner = lrn(\"classif.xgboost\", predict_type = \"prob\"),\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.auc\"),\n  search_space = ps(\n    eta = p_dbl(lower = 0.001, upper = 1),\n    nrounds = p_int(lower = 1, upper = 500)\n  ),\n  terminator = trm(\"evals\", n_evals = 50),\n  tuner = tnr(\"random_search\")\n)\n\n# Tune!\ntuned_xgboost$train(spam_task, row_ids = spam_split$train)\n\nautoplot(tuned_xgboost$tuning_instance, cols_x = c(\"nrounds\", \"eta\"))\n```\n\n::: {.cell-output-display}\n![](04-tuning_files/figure-html/example-tuned-xgboost-1.png){fig-align='center' width=672}\n:::\n\n```{.r .cell-code}\n# Evaluate!\ntuned_xgboost$predict(spam_task, row_ids = spam_split$test)$score(msr(\n  \"classif.auc\"\n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> classif.auc \n#>   0.9876319\n```\n\n\n:::\n:::\n\n\n#### `ranger` Tuning\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Tuning setup\ntuned_ranger = auto_tuner(\n  learner = lrn(\"classif.ranger\", predict_type = \"prob\"),\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.auc\"),\n  search_space = ps(\n    mtry.ratio = p_dbl(lower = 0, upper = 1),\n    max.depth = p_int(lower = 1, upper = 30)\n  ),\n  terminator = trm(\"evals\", n_evals = 50),\n  tuner = tnr(\"random_search\")\n)\n\n# Tune!\ntuned_ranger$train(spam_task, row_ids = spam_split$train)\n\n# Evaluate!\ntuned_ranger$predict(spam_task, row_ids = spam_split$test)$score(msr(\n  \"classif.auc\"\n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> classif.auc \n#>   0.9834206\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check parameter results\nautoplot(tuned_ranger$tuning_instance)\n```\n\n::: {.cell-output-display}\n![](04-tuning_files/figure-html/example-tuned-ranger-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n:::\n\n## Benchmarking all the things (with tuning)\n\nAbove we tuned all the learners individually, but often we want to tune all of them at the same time to determine which performs best overall. For that, we use `benchmark_grid()` again (like in the second notebook), but now we just give it the `AutoTuner`-style learners instead of the \"normal\" learners.\n\nSince we have already set up the tuning-ready learners (`tuned_<method>` objects) above we just recycle them here, but we first reset all of them since we already tuned them and we want to start from scratch.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntuned_knn$reset()\ntuned_rpart$reset()\ntuned_ranger$reset()\ntuned_xgboost$reset()\n\ntuning_learners <- list(\n  tuned_knn,\n  tuned_rpart,\n  tuned_ranger,\n  tuned_xgboost\n)\n\ntuning_benchmark_design <- benchmark_grid(\n  tasks = spam_task, # Still the same task. Optional: Use list() of multiple tasks for large benchmark study\n  learners = tuning_learners, # List of AutoTune-learners\n  resamplings = rsmp(\"holdout\") # Outer resampling strategy, holdout to keep it simpel\n)\n\n# Run the benchmark and save the results\nfuture::plan(\"multisession\", workers = 4)\nbmr <- benchmark(tuning_benchmark_design)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Growing trees.. Progress: 28%. Estimated remaining time: 25 minutes, 3 seconds.\n```\n\n\n:::\n\n```{.r .cell-code}\n# Who won?\nbmr$aggregate(msr(\"classif.auc\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>       nr task_id            learner_id resampling_id iters classif.auc\n#>    <int>  <char>                <char>        <char> <int>       <num>\n#> 1:     1    spam    classif.kknn.tuned       holdout     1   0.9607047\n#> 2:     2    spam   classif.rpart.tuned       holdout     1   0.9405357\n#> 3:     3    spam  classif.ranger.tuned       holdout     1   0.9803224\n#> 4:     4    spam classif.xgboost.tuned       holdout     1   0.9823780\n#> Hidden columns: resample_result\n```\n\n\n:::\n:::\n\n\nFor statistical tests on benchmark results, refer to the `mlr3benchmark` package (not included with `mlr3verse`).\n\n:::\n\n\n\n::: {.callout-tip title=\"Not sure what to tune?\"}\n\nWhich parameter to tune in which interval is usually an area of research, unless you have specific theory- or domain-driven constraints.  \n\nThe [mlr3tuningspaces](https://mlr3tuningspaces.mlr-org.com/) packages aims to collect comprehensive tuning spaces for many learners, and when in doubt, they are usually a good place to start.\nNote that it's most likely not necessary to extensively tune all parameters as much as possible --- diminishing returns and such!\n\n:::\n",
    "supporting": [
      "04-tuning_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}