{
  "hash": "632e8f823ad9e37837197f3a68dd88d1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'kNN & Trees'\neditor_options:\n  chunk_output_type: console\n---\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggplot2) # For plotting\nlibrary(kknn) # For kNN learner\nlibrary(rpart) # For decision tree learners\nlibrary(palmerpenguins) # For penguins\n```\n:::\n\n\nGoals of this part:\n\n1.  Taking a look at our example dataset\n2.  Introduce kNN via `{kknn}` and decision trees via `{rpart}`\n3.  Train some models, look at some results\n\n# The dataset: Pengiuns!\n\nSee [their website](https://allisonhorst.github.io/palmerpenguins/) for some more information if you're interested.  \nFor now it's enough to know that we have a bunch of data about 3 species of penguins.\n\n![](img/lter_penguins.png)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# remove missing values for simplicity in this example\n# (handling missing data is a can of worms for another time :)\npenguins <- na.omit(penguins)\n\nstr(penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> tibble [333 Ã— 8] (S3: tbl_df/tbl/data.frame)\n#>  $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n#>  $ bill_length_mm   : num [1:333] 39.1 39.5 40.3 36.7 39.3 38.9 39.2 41.1 38.6 34.6 ...\n#>  $ bill_depth_mm    : num [1:333] 18.7 17.4 18 19.3 20.6 17.8 19.6 17.6 21.2 21.1 ...\n#>  $ flipper_length_mm: int [1:333] 181 186 195 193 190 181 195 182 191 198 ...\n#>  $ body_mass_g      : int [1:333] 3750 3800 3250 3450 3650 3625 4675 3200 3800 4400 ...\n#>  $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 1 2 1 2 1 2 2 ...\n#>  $ year             : int [1:333] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n#>  - attr(*, \"na.action\")= 'omit' Named int [1:11] 4 9 10 11 12 48 179 219 257 269 ...\n#>   ..- attr(*, \"names\")= chr [1:11] \"4\" \"9\" \"10\" \"11\" ...\n```\n\n\n:::\n:::\n\n\n![](img/culmen_depth.png)\n\nWe can take a look at the different species across two numeric features, starting with flipper length and body mass (for reasons that may become clear later):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g, fill = species)) +\n  # ggforce::geom_mark_ellipse(aes(fill = species, label = species)) +\n  geom_point(\n    shape = 21,\n    stroke = 1 / 4,\n    size = 3,\n    alpha = 2 / 3,\n    key_glyph = \"rect\"\n  ) +\n  labs(\n    title = \"Palmer Penguins\",\n    subtitle = \"Body mass and flipper length by species\",\n    x = \"Flipper Length [mm]\",\n    y = \"Body Mass [g]\",\n    color = \"Species\",\n    fill = \"Species\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    legend.position = \"bottom\",\n    plot.title.position = \"plot\"\n  )\n```\n\n::: {.cell-output-display}\n![](01-knn-trees_files/figure-html/penguins-plot-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n::: {.callout-note}\nThere are two `penguins` datasets:\n\n- `penguins` from the `palmerpenguins` R package, which has been around for a while\n- `penguins`, as included in R's `datasets` package as of recently, with slightly different variable names!\n\nFor backwards compatbility, we just assume the `palmerpenguins` version.  \nIn the future, we will probably [convert these materials](https://cran.r-project.org/web/packages/basepenguins/vignettes/basepenguins.html)\n\n:::\n\nWe split our penguin data in a roughly 2/3 to 1/3 training- and test dataset for our first experiments:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npenguin_N <- nrow(penguins) # Our total sample size\n\n# We draw 2/3 of all indices randomly with a sampling proportion of 2/3 with a fixed seed\nset.seed(234)\ntrain_ids <- sample(penguin_N, replace = FALSE, size = penguin_N * 2 / 3)\n# Our test set are the indices not in the training set\ntest_ids <- setdiff(1:penguin_N, train_ids)\n\n# Assemble our train/test set using the indices we just randomized\npenguins_train <- penguins[train_ids, ]\npenguins_test <- penguins[test_ids, ]\n```\n:::\n\n\n# kNN and trees, step by step\n\nNow that we have some data, we can start fitting models, just to see how it goes!\nGiven the plot from earlier, we may have a rough idea that the flipper length and body mass measurements are already giving us a somewhat decent picture about the species.\n\n## kNN\n\nLet's start with the nearest-neighbor approach via the `{kknn}` package.\nIt takes a `formula` argument like you may know from `lm()` and other common modeling functions in R, where the format is `predict_this ~ on_this + and_that + ...`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nknn_penguins <- kknn(\n  formula = species ~ flipper_length_mm + body_mass_g,\n  k = 3, # Hyperparameter: How many neighbors to consider\n  train = penguins_train, # Training data used to make predictions\n  test = penguins_test # Data to make predictions on\n)\n\n# Peek at the predictions, one row per observation in test data\nhead(knn_penguins$prob)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>         Adelie Chinstrap Gentoo\n#> [1,] 1.0000000 0.0000000      0\n#> [2,] 0.3333333 0.6666667      0\n#> [3,] 1.0000000 0.0000000      0\n#> [4,] 0.3333333 0.6666667      0\n#> [5,] 1.0000000 0.0000000      0\n#> [6,] 1.0000000 0.0000000      0\n```\n\n\n:::\n:::\n\n\nTo get an idea of how well our predictions fit, we add them to our original test data and compare observed (true) and predicted species:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Add predictions to the test dataset\npenguins_test$knn_predicted_species <- fitted(knn_penguins)\n\n# Rows: True species, columns: predicted species\ntable(\n  penguins_test$species,\n  penguins_test$knn_predicted_species,\n  dnn = c(\"Observed\", \"Predicted\")\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>            Predicted\n#> Observed    Adelie Chinstrap Gentoo\n#>   Adelie        38         8      1\n#>   Chinstrap     12         8      4\n#>   Gentoo         0         1     39\n```\n\n\n:::\n:::\n\n\nProportion of correct predictions (\"accuracy\")...\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmean(penguins_test$species == penguins_test$knn_predicted_species)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.7657658\n```\n\n\n:::\n:::\n\n\n...and **incorrect** predictions (classification error):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmean(penguins_test$species != penguins_test$knn_predicted_species)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.2342342\n```\n\n\n:::\n:::\n\n\n\n::: {.callout-note title=\"R shortcut\"} \nLogical comparison gives logical vector of `TRUE`/`FALSE`, which can be used like 1 / 0 for mathematical operations, so we can sum up cases where `observed == predicted` (=> correct classifications) and divide by N for the proportion, i.e. calculate the proportion of correct predictions, the accuracy.\n:::\n\n## Your turn!\n\nAbove you have working code for an acceptable but *not great* kNN model.\nCan you make it even better?\nCan you change something to make it *worse*?\n\nSome things to try:\n\n1.  Try different predictors, maybe leave some out\n    - Which seem to work best?\n\n2.  Try using all available predictors (`formula = species ~ .`)\n    - Would you recommend doing that? Does it work well?\n\n3.  Try different `k` values.\n    Is higher == better? (You can stick to odd numbers)\n    - After you've tried a couple `k`'s, does it get cumbersome yet?\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# your code\nknn_penguins <- kknn(\n  formula = species ~ .,\n  k = 7, # Hyperparameter: How many neighbors to consider\n  train = penguins_train, # Training data used to make predictions\n  test = penguins_test # Data to make predictions on\n)\npenguins_test$knn_predicted_species <- fitted(knn_penguins)\nmean(penguins_test$species == penguins_test$knn_predicted_species)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 1\n```\n\n\n:::\n:::\n\n\n\n::: {.callout-tip title=\"Example solution\" collapse =\"true\"}\n\nAlready perfect accuracy with multiple predictors, not so interesting\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nknn_penguins <- kknn(\n  formula = species ~ flipper_length_mm + body_mass_g + bill_length_mm + bill_depth_mm,\n  k = 5,\n  train = penguins_train,\n  test = penguins_test\n)\n\npenguins_test$knn_predicted_species <- fitted(knn_penguins)\nmean(penguins_test$species == penguins_test$knn_predicted_species)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 1\n```\n\n\n:::\n:::\n\n\nThe \"try a bunch of `k`\"-shortcut function (using less features to be moderately interesting):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nknn_try_k <- function(k) {\n  knn_penguins <- kknn::kknn(\n    formula = species ~ flipper_length_mm + body_mass_g + bill_depth_mm,\n    k = k,\n    train = penguins_train,\n    test = penguins_test\n  )\n\n  penguins_test$knn_predicted_species <- fitted(knn_penguins)\n  acc <- mean(penguins_test$species == penguins_test$knn_predicted_species)\n  \n  data.frame(k = k, accuracy = acc)\n}\n\n# Call function ^ with k = 1 through 10, collect result as data.frame\nk_result <- do.call(rbind, lapply(1:10, knn_try_k))\n\nggplot(k_result, aes(x = k, y = accuracy)) +\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = 1:10) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(\n    title = \"kNN test accuracy on penguin dataset\",\n    x = \"k\", y = \"Test accuracy\"\n  ) +\n  theme_minimal() +\n  theme(panel.grid.minor = element_blank())\n```\n\n::: {.cell-output-display}\n![](01-knn-trees_files/figure-html/example-knn-penguins-looptune-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nThis is a cumbersome way to find the \"best\" `k` though --- we'll learn about the better ways later!\n\n(We basically did a *grid search* across `k` here)\n\n:::\n\n\n\n## Growing a decision tree\n\nNow that we've played around with kNN a little, let's grow some trees!\nWe'll use the `{rpart}` (**R**ecursive **Part**itioning) package and start with the same model specification as before and use the default parameters.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrpart_penguins <- rpart(\n  formula = species ~ flipper_length_mm + body_mass_g,\n  data = penguins_train, # Train data\n  method = \"class\", # Grow a classification tree (don't change this for now)\n)\n```\n:::\n\n\nThe nice thing about a single tree is that you can just look at it and know exactly what it did:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrpart_penguins\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> n= 222 \n#> \n#> node), split, n, loss, yval, (yprob)\n#>       * denotes terminal node\n#> \n#>  1) root 222 123 Adelie (0.445945946 0.198198198 0.355855856)  \n#>    2) flipper_length_mm< 206.5 141  43 Adelie (0.695035461 0.297872340 0.007092199)  \n#>      4) flipper_length_mm< 194.5 96  19 Adelie (0.802083333 0.197916667 0.000000000) *\n#>      5) flipper_length_mm>=194.5 45  22 Chinstrap (0.466666667 0.511111111 0.022222222)  \n#>       10) body_mass_g>=3762.5 29  12 Adelie (0.586206897 0.379310345 0.034482759)  \n#>         20) flipper_length_mm< 200.5 16   3 Adelie (0.812500000 0.187500000 0.000000000) *\n#>         21) flipper_length_mm>=200.5 13   5 Chinstrap (0.307692308 0.615384615 0.076923077) *\n#>       11) body_mass_g< 3762.5 16   4 Chinstrap (0.250000000 0.750000000 0.000000000) *\n#>    3) flipper_length_mm>=206.5 81   3 Gentoo (0.012345679 0.024691358 0.962962963) *\n```\n\n\n:::\n:::\n\n\nLooking at the tree as a... tree.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(rpart_penguins)\ntext(rpart_penguins)\n```\n\n::: {.cell-output-display}\n![](01-knn-trees_files/figure-html/tree-model-plot-base-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nMuch nicer to use the `rpart.plot` package though\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(rpart.plot)\nrpart.plot(rpart_penguins)\n```\n\n::: {.cell-output-display}\n![](01-knn-trees_files/figure-html/tree-model-plot-color-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nIf we want to know how accurate we are with our model we need to make predictions on our test data manually:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrpart_predictions <- predict(\n  rpart_penguins, # The model we just fit\n  newdata = penguins_test, # New data to predict species on\n  type = \"class\" # We want class predictions (the species), not probabilities\n)\n\npenguins_test$rpart_predicted_species <- rpart_predictions\n\n# Same procedure as with kNN before\ntable(penguins_test$species, penguins_test$rpart_predicted_species)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>            \n#>             Adelie Chinstrap Gentoo\n#>   Adelie        40         6      1\n#>   Chinstrap     14         7      3\n#>   Gentoo         0         0     40\n```\n\n\n:::\n\n```{.r .cell-code}\n# And our accuracy score\nmean(penguins_test$species == penguins_test$rpart_predicted_species)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.7837838\n```\n\n\n:::\n:::\n\n\n## Your turn!\n\nWe haven't picked any hyperparameter settings for our tree yet, maybe we should try?\n\n1.  What hyperparameters does `rpart()` offer?\n    Do you recognize some from the lecture?\n    - You can check via `?rpart.control`\n    - When in doubt check `minsplit`, `maxdepth` and `cp`\n\n2.  Try out trees with different parameters\n    - Would you prefer simple or complex trees?\n    - How far can you improve the tree's accuracy?\n\nSo, what seems to work better here?  \nkNN or trees?\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# your code\n```\n:::\n\n\n::: {.callout-tip title=\"Example solution\" collapse =\"true\"}\n\nA \"full\" call to rpart with (most) relevant hyperparameters\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrpart_penguins <- rpart(\n  formula = species ~ flipper_length_mm + body_mass_g,\n  data = penguins_train, # Train data\n  method = \"class\", # Grow a classification tree (don't change this)\n  cp = 0.03, # Complexity parameter for regularization (default = 0.01)\n  minsplit = 15, # Number of obs to keep in node to continue splitting, default = 20\n  minbucket = 2, # Number of obs to keep in terminal/leaf nodes, default is minsplit/3\n  maxdepth = 15 # Maximum tree depth, default (and upper limit for rpart!) = 30\n)\n\n# Evaluate\npenguins_test$rpart_predicted_species <- rpart_predictions\n# Confusion matrix\ntable(penguins_test$species, penguins_test$rpart_predicted_species)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>            \n#>             Adelie Chinstrap Gentoo\n#>   Adelie        40         6      1\n#>   Chinstrap     14         7      3\n#>   Gentoo         0         0     40\n```\n\n\n:::\n\n```{.r .cell-code}\n# Accuracy\nmean(penguins_test$species == penguins_test$rpart_predicted_species)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.7837838\n```\n\n\n:::\n:::\n\n\nNot much else to show here, just play around with the parameters!\n\n:::\n\n\n## Plotting decision boundaries (for 2 predictors)\n\nThis is a rather cumbersome manual approach --- there's a nicer way we'll see later, but we'll do it the manual way at least once so you know how it works:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Decision tree to plot the boundaries of\nrpart_penguins <- rpart(\n  formula = species ~ flipper_length_mm + body_mass_g,\n  data = penguins_train, # Train data\n  method = \"class\", # Grow a classification tree (don't change this)\n  cp = 0.005, # Default 0.01\n  minsplit = 20, # Default 20\n  minbucket = 3, # Default is minsplit/3\n  maxdepth = 30 # Default 30 (and upper limit!)\n)\n\n# Ranges of X and Y variable on plot\nflipper_range <- range(penguins$flipper_length_mm)\nmass_range <- range(penguins$body_mass_g)\n\n# A grid of values within these boundaries, 100 points per axis\npred_grid <- expand.grid(\n  flipper_length_mm = seq(flipper_range[1], flipper_range[2], length.out = 100),\n  body_mass_g = seq(mass_range[1], mass_range[2], length.out = 100)\n)\n\n# Predict with tree for every single point\npred_grid$rpart_prediction <- predict(\n  rpart_penguins,\n  newdata = pred_grid,\n  type = \"class\"\n)\n\n# Plot all predictions, colored by species\nggplot(pred_grid, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_tile(\n    aes(color = rpart_prediction, fill = rpart_prediction),\n    linewidth = 1,\n    show.legend = FALSE\n  ) +\n  geom_point(\n    data = penguins_test,\n    aes(fill = species),\n    shape = 21,\n    color = \"black\",\n    size = 2,\n    key_glyph = \"rect\"\n  ) +\n  labs(\n    title = \"Palmer Penguins: Decision Boundaries\",\n    subtitle = paste(\n      \"Species as predicted by decision tree\",\n      \"Point color is the true species\",\n      sep = \"\\n\"\n    ),\n    x = \"Flipper Length [mm]\",\n    y = \"Body Mass [g]\",\n    fill = \"Species\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    legend.position = \"top\",\n    plot.title.position = \"plot\",\n    panel.grid = element_blank()\n  )\n```\n\n::: {.cell-output-display}\n![](01-knn-trees_files/figure-html/decision-boundary-plot-1.png){fig-align='center' width=672}\n:::\n:::\n\n",
    "supporting": [
      "01-knn-trees_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}