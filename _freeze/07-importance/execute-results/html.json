{
  "hash": "76a2286b3147df571d0ab425c77ae0c1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Feature Importance\"\neditor_options: \n  chunk_output_type: console\n---\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse) # All the mlr3 things\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Loading required package: mlr3\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(effectplots) # For effects plotting\n\n# Penguin Task setup\npenguins <- na.omit(palmerpenguins::penguins)\npenguin_task <- as_task_classif(\n  penguins,\n  target = \"species\"\n)\n\nlgr::get_logger(\"mlr3\")$set_threshold(\"error\")\n```\n:::\n\n\nGoals of this part:\n\n1. Introduce (some) variable importance measures\n2. Compare measures for different settings\n\n# Feature Importance\n\nFeature importance falls under the umbrella of interpretability, which is a huge topic, and there's a lot to explore --- we'll cover some basics and if you're interested, you can always find more at\n\n- The [IML lecture](https://slds-lmu.github.io/iml/) (free slides, videos)\n- The [Interpretable Machine Learning book](https://christophm.github.io/interpretable-ml-book)\n- The [`{mlr3}` book chapter](https://mlr3book.mlr-org.com/interpretation.html)\n\nBefore we get started with the general methods, it should be noted that some learners bring their own method-specific importance measures.\nRandom Forests (via `{ranger}`) for example has some built-in importance\nmetrics, like the corrected Gini impurity:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_ranger <- lrn(\"classif.ranger\", importance = \"impurity_corrected\")\nlrn_ranger$train(penguin_task)\n\nsort(lrn_ranger$importance(), decreasing = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>    bill_length_mm flipper_length_mm     bill_depth_mm       body_mass_g \n#>        54.8458893        38.9815057        26.2726828        22.9065297 \n#>            island               sex              year \n#>        21.3959117         0.7257007         0.1781184\n```\n\n\n:::\n:::\n\n\nWhich shows us that for our penguins, `bill_length_mm` is probably the most relevant feature, whereas `body_mass_g` does not turn out to be as important with regard to species classification.\nTHe `year` doesn't seem to be useful for prediction at all --- which is perfectly plausible!\n\n\n## Feature Importance with `{mlr3filters}`\n\nThe `mlr3filters` package provides some _global_, _marginal_ importance methods, meaning they consider the relationship between the target and one feature at a time. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(mlr_filters)[1:20, .(key, label)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Key: <key>\n#>                  key                                                    label\n#>               <char>                                                   <char>\n#>  1:            anova                                             ANOVA F-Test\n#>  2:              auc                           Area Under the ROC Curve Score\n#>  3:           boruta                                                   Burota\n#>  4:         carscore                   Correlation-Adjusted coRrelation Score\n#>  5:     carsurvscore          Correlation-Adjusted coRrelation Survival Score\n#>  6:             cmim      Minimal Conditional Mutual Information Maximization\n#>  7:      correlation                                              Correlation\n#>  8:             disr                       Double Input Symmetrical Relevance\n#>  9: find_correlation                                  Correlation-based Score\n#> 10:       importance                                         Importance Score\n#> 11: information_gain                                         Information Gain\n#> 12:              jmi                                 Joint Mutual Information\n#> 13:             jmim            Minimal Joint Mutual Information Maximization\n#> 14:     kruskal_test                                      Kruskal-Wallis Test\n#> 15:              mim                          Mutual Information Maximization\n#> 16:             mrmr                     Minimum Redundancy Maximal Relevancy\n#> 17:            njmim Minimal Normalised Joint Mutual Information Maximization\n#> 18:      performance                                   Predictive Performance\n#> 19:      permutation                                        Permutation Score\n#> 20:           relief                                                   RELIEF\n#>                  key                                                    label\n```\n\n\n:::\n:::\n\n\nOne \"trick\" of the filters package is that it can be used to access the `$importance()` that some learners provide on their own, and `ranger` provides the impurtiy-importance and permutation feature importance (PFI). \nWe can access either with `mlr3filters` directly, but note it retrains the learner:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_ranger <- lrn(\"classif.ranger\", importance = \"impurity_corrected\")\nfilter_importance = flt(\"importance\", learner = lrn_ranger)\nfilter_importance$calculate(penguin_task)\n\nfilter_importance\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> <FilterImportance:importance>: Importance Score\n#> Task Types: classif\n#> Properties: missings\n#> Task Properties: -\n#> Packages: mlr3, mlr3learners, ranger\n#> Feature types: logical, integer, numeric, character, factor, ordered\n#>              feature       score\n#> 1:    bill_length_mm 52.26957520\n#> 2: flipper_length_mm 31.16740779\n#> 3:     bill_depth_mm 28.03858992\n#> 4:            island 23.40626716\n#> 5:       body_mass_g 22.16776584\n#> 6:              year  0.01123963\n#> 7:               sex -0.22059306\n```\n\n\n:::\n:::\n\n\n`mlr3filters` also provides a general implementation for PFI that retrains the learner repeatedly with one feature randomly shuffled.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_ranger <- lrn(\"classif.ranger\")\nfilter_permutation = flt(\"permutation\", learner = lrn_ranger)\nfilter_permutation$calculate(penguin_task)\n\nfilter_permutation\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> <FilterPermutation:permutation>: Permutation Score\n#> Task Types: classif\n#> Properties: missings\n#> Task Properties: -\n#> Packages: mlr3, mlr3learners, ranger, mlr3measures\n#> Feature types: logical, integer, numeric, character, factor, ordered\n#>              feature      score\n#> 1:    bill_length_mm 1.00000000\n#> 2: flipper_length_mm 0.10595065\n#> 3:               sex 0.09869376\n#> 4:            island 0.08272859\n#> 5:     bill_depth_mm 0.07837446\n#> 6:       body_mass_g 0.07547170\n#> 7:              year 0.03193033\n```\n\n\n:::\n:::\n\n\nBut that also means we can use PFI for _any other_ learner, such the SVM or XGBoost!\n\n## Your Turn!\n\n- Compute PFI with `ranger` using it's built-in `$importance()`, using `mlr3filters`\n- Compute PFI for an SVM using the approproate `\"permutation\"` filter\n- Comapre the two methods. Do they agree?\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Your code\n```\n:::\n\n\n\n::: {.callout-tip title=\"mlr3 preprocessing pipelines\"}\n\nNote that we need to use the encoding `PipeOp` to train the SVM of these on the penguins task, as they can't handle categorical features automatically:\n\n```r\nlrn_svm <- po(\"encode\") %>>%\n  po(\"learner\", lrn(\"classif.svm\", kernel = \"radial\", <any other parameter>)) |>\n  as_learner()\n```\n\n:::\n\n\n::: {.callout-tip title=\"Example solution\" collapse =\"true\"}\n\nThis is just for demonstration --- we'd need to use tuned hyperparameters for the SVM for a proper comparison!\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npenguin_task <- as_task_classif(\n  na.omit(palmerpenguins::penguins),\n  target = \"species\"\n)\n\nlrn_svm <- po(\"encode\") %>>%\n  po(\"learner\", lrn(\"classif.svm\")) |>\n  as_learner()\n\npfi_svm = flt(\"permutation\", learner = lrn_svm)\npfi_svm$calculate(penguin_task)\npfi_svm\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> <FilterPermutation:permutation>: Permutation Score\n#> Task Types: classif\n#> Properties: missings\n#> Task Properties: -\n#> Packages: mlr3, mlr3pipelines, stats, mlr3learners, e1071, mlr3measures\n#> Feature types: logical, integer, numeric, character, factor, ordered,\n#>   POSIXct, Date\n#>              feature        score\n#> 1:    bill_length_mm  1.000000000\n#> 2:            island  0.059701493\n#> 3:     bill_depth_mm  0.000000000\n#> 4:               sex -0.006784261\n#> 5: flipper_length_mm -0.016282225\n#> 6:              year -0.021709634\n#> 7:       body_mass_g -0.032564450\n```\n\n\n:::\n\n```{.r .cell-code}\npfi_ranger = flt(\"importance\", learner = lrn(\"classif.ranger\", importance = \"permutation\"))\npfi_ranger$calculate(penguin_task)\npfi_ranger\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> <FilterImportance:importance>: Importance Score\n#> Task Types: classif\n#> Properties: missings\n#> Task Properties: -\n#> Packages: mlr3, mlr3learners, ranger\n#> Feature types: logical, integer, numeric, character, factor, ordered\n#>              feature         score\n#> 1:    bill_length_mm  0.2503163373\n#> 2: flipper_length_mm  0.1715246117\n#> 3:     bill_depth_mm  0.1230873109\n#> 4:            island  0.1167507953\n#> 5:       body_mass_g  0.0775342373\n#> 6:               sex  0.0120370742\n#> 7:              year -0.0001833529\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n\n\n## Feature Effects with `{effectplots}`\n\nGetting a number for \"is this feature important\" is nice, but often we want a better picture of the feature's effect. Think of linear models and how we can interpret $\\beta_j$ as the linear relationship between $X_j$ and $Y$ --- often things aren't linear though.\n\nOne approach to visualize feature effects is via *Partial Dependence Plots* or preferably via\n*Accumulated Local Effect* plots (ALE), which we get from the `{effectplots}` thankfully offers.\n\nLet's recycle our `ranger` learner and plot some effects, using the partial dependence plot (PDP) as an example:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_ranger_cl <- lrn(\"classif.ranger\", predict_type = \"prob\")\nlrn_ranger_cl$train(penguin_task)\n\npd_penguins <- partial_dependence(\n  object = lrn_ranger_cl$model,\n  v = penguin_task$feature_names,\n  data = penguin_task$data(),\n  which_pred = \"Adelie\"\n)\n\nplot(pd_penguins)\n```\n\n::: {.cell-output-display}\n![](07-importance_files/figure-html/feature-effects-ale-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n::: {.callout-note}\nWe're doing _multiclass classification_ here, so while our learner predicts a probability for one of each of the three target classes (Adelie, Gentoo, Chinstrap), we need to pick one for the visualization!\n:::\n\n## Your turn! (Possibly for another time)\n\n- Use the `bike_share` regression task to calculate the PDP\n- Stick with the `ranger` learner as `{effectplots}` supports it directly.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# your code\n```\n:::\n\n\n::: {.callout-tip title=\"Example solution\" collapse =\"true\"}\n\nThe `bike_sharing` task is a regression task, so make sure to switch to the regression version of the learner. \n\nThe target is `bikers`, the number of people using a specific bike sharing service. More information can be found on the [UCI website](https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask <- tsk(\"bike_sharing\")\nlrn_ranger <- lrn(\"regr.ranger\")\nlrn_ranger$train(task)\n\npd_bikeshare <- partial_dependence(\n  object = lrn_ranger$model,\n  v = task$feature_names,\n  data = task$data()\n)\n\nplot(pd_bikeshare)\n```\n\n::: {.cell-output-display}\n![](07-importance_files/figure-html/pdp-bikeshare-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n:::\n\n",
    "supporting": [
      "07-importance_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}