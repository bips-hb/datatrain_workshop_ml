{
  "hash": "2dd130d2bdf2c0577b28c9e6245eb128",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Resampling, Random Forest & Boosting\"\neditor_options: \n  chunk_output_type: console\n---\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlr3verse) # Loads all the mlr3 stuff\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: mlr3\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(ggplot2)   # For plotting\n\n# Just telling mlr3 to be quiet unless something broke\nlgr::get_logger(\"mlr3\")$set_threshold(\"error\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"error\")\n```\n:::\n\n\nGoals of this part:\n\n1. A Quick look at other tasks available and the \"spam\" task specifically\n2. Resampling for model evaluation\n3. Comparing learners with resampling\n\n# Task Setup\n\nAs we've seen last time, our penguin data is pretty easy for our learners.\nWe need something a little more complex, meaning more observations (n)\nand a couple more predictors (p).  \nIf you're looking for ready-made example tasks for your own experimentation,\n`mlr3` comes with a couple you can try. The procedure is similar to `mlr_learners`\nand `mlr_measures`, but this time it's, you guessed it,  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmlr_tasks\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<DictionaryTask> with 22 stored values\nKeys: ames_housing, bike_sharing, boston_housing, breast_cancer,\n  california_housing, german_credit, ilpd, iris, kc_housing, moneyball,\n  mtcars, optdigits, penguins, penguins_simple, pima, ruspini, sonar,\n  spam, titanic, usarrests, wine, zoo\n```\n\n\n:::\n\n```{.r .cell-code}\n# As a neat little table with all the relevant info\nas.data.table(mlr_tasks)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nKey: <key>\n                   key                                     label task_type\n                <char>                                    <char>    <char>\n 1:       ames_housing                          Ames House Sales      regr\n 2:       bike_sharing                       Bike Sharing Demand      regr\n 3:     boston_housing                     Boston Housing Prices      regr\n 4:      breast_cancer                   Wisconsin Breast Cancer   classif\n 5: california_housing                    California House Value      regr\n 6:      german_credit                             German Credit   classif\n 7:               ilpd                 Indian Liver Patient Data   classif\n 8:               iris                              Iris Flowers   classif\n 9:         kc_housing                   King County House Sales      regr\n10:          moneyball          Major League Baseball Statistics      regr\n11:             mtcars                              Motor Trends      regr\n12:          optdigits Optical Recognition of Handwritten Digits   classif\n13:           penguins                           Palmer Penguins   classif\n14:    penguins_simple                Simplified Palmer Penguins   classif\n15:               pima                      Pima Indian Diabetes   classif\n16:            ruspini                                   Ruspini     clust\n17:              sonar                    Sonar: Mines vs. Rocks   classif\n18:               spam                         HP Spam Detection   classif\n19:            titanic                                   Titanic   classif\n20:          usarrests                                US Arrests     clust\n21:               wine                              Wine Regions   classif\n22:                zoo                               Zoo Animals   classif\n                   key                                     label task_type\n     nrow  ncol properties   lgl   int   dbl   chr   fct   ord   pxc   dte\n    <int> <int>     <list> <int> <int> <int> <int> <int> <int> <int> <int>\n 1:  2930    82                0    33     1     0    47     0     0     0\n 2: 17379    14                2     4     4     1     2     0     0     0\n 3:   506    18                0     3    12     0     2     0     0     0\n 4:   683    10   twoclass     0     0     0     0     0     9     0     0\n 5: 20640    10                0     0     8     0     1     0     0     0\n 6:  1000    21   twoclass     0     3     0     0    14     3     0     0\n 7:   583    11   twoclass     0     4     5     0     1     0     0     0\n 8:   150     5 multiclass     0     0     4     0     0     0     0     0\n 9: 21613    20                1    13     4     0     0     0     1     0\n10:  1232    15                0     3     5     0     6     0     0     0\n11:    32    11                0     0    10     0     0     0     0     0\n12:  5620    65   twoclass     0    64     0     0     0     0     0     0\n13:   344     8 multiclass     0     3     2     0     2     0     0     0\n14:   333    11 multiclass     0     3     7     0     0     0     0     0\n15:   768     9   twoclass     0     0     8     0     0     0     0     0\n16:    75     2                0     2     0     0     0     0     0     0\n17:   208    61   twoclass     0     0    60     0     0     0     0     0\n18:  4601    58   twoclass     0     0    57     0     0     0     0     0\n19:  1309    11   twoclass     0     2     2     3     2     1     0     0\n20:    50     4                0     2     2     0     0     0     0     0\n21:   178    14 multiclass     0     2    11     0     0     0     0     0\n22:   101    17 multiclass    15     1     0     0     0     0     0     0\n     nrow  ncol properties   lgl   int   dbl   chr   fct   ord   pxc   dte\n```\n\n\n:::\n:::\n\n\nI suggest we try a two-class (i.e. binary) classification problem next, and\nmaybe something we can probably all somewhat relate to: **Spam detection**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspam_task <- tsk(\"spam\")\n\n# Outcome categories\nspam_task$class_names\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"spam\"    \"nonspam\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Target classes are not horribly unbalanced (40/60%)\nautoplot(spam_task)\n```\n\n::: {.cell-output-display}\n![](02-resampling-rf-boosting_files/figure-html/task-spam-1.png){width=672}\n:::\n:::\n\n\n## Your turn!\n\nExplore the task a little (you can use its built-in methods `spam_task$...`)\nto check the data, get column types, the dataset dimensions etc.\nThink about this as a new analysis problem, so what do we need to know here?\n\n(If you'd like a simpler overview, you can read the help with `spam_task$help()`)\n\n# Resampling\n\nTo continue with our kNN and tree experiments, we'll now enter resampling territory.  \nYou probably realized that predicting on just one test dataset doesn't give us\ntoo much of a useful idea of our model, which is why we use resampling.\n\nThere are [lots of resampling strategies](https://mlr3.mlr-org.com/reference/index.html#resampling-strategies), \nbut you usually can't go too wrong with cross validation (CV), which should not \ntake too much time and computing power on small datasets like ours.\n\nInstead of training our model just once, we're going to train it 5 times with \ndifferent training- and test-datasets. In `{mlr3}`, we first pick a resampling\nstrategy with `rsmp()`, and then define our setup with `resample()` based on\nour task and resampling strategy. The result an object of class `ResampleResult`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrr <- resample(\n  task = spam_task, \n  # Optional: Adjust learner parameters\n  learner = lrn(\"classif.kknn\", k = 13, predict_sets = c(\"train\", \"test\")),\n  resampling = rsmp(\"cv\", folds = 3) # 3-fold CV\n)\n\nrr\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n── <ResampleResult> with 3 resampling iterations ───────────────────────────────\n task_id   learner_id resampling_id iteration    prediction_train\n    spam classif.kknn            cv         1 <PredictionClassif>\n    spam classif.kknn            cv         2 <PredictionClassif>\n    spam classif.kknn            cv         3 <PredictionClassif>\n     prediction_test warnings errors\n <PredictionClassif>        0      0\n <PredictionClassif>        0      0\n <PredictionClassif>        0      0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Contains task/learner/resampling/prediction objects\nas.data.table(rr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 task                           learner     resampling\n               <list>                            <list>         <list>\n1: <TaskClassif:spam> <LearnerClassifKKNN:classif.kknn> <ResamplingCV>\n2: <TaskClassif:spam> <LearnerClassifKKNN:classif.kknn> <ResamplingCV>\n3: <TaskClassif:spam> <LearnerClassifKKNN:classif.kknn> <ResamplingCV>\n   iteration          prediction\n       <int>              <list>\n1:         1 <PredictionClassif>\n2:         2 <PredictionClassif>\n3:         3 <PredictionClassif>\n```\n\n\n:::\n:::\n\n\nWe also tell our learner to predict both on train- and test sets.  \nNow we have a look at our predictions, but *per resampling iteration*, for both\ntrain- and test-set accuracy:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntraintest_acc <- list(\n  msr(\"classif.acc\", predict_sets = \"train\", id = \"acc_train\"),\n  msr(\"classif.acc\", id = \"acc_test\")\n)\nrr$score(traintest_acc)[, .(iteration, acc_train, acc_test)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   iteration acc_train  acc_test\n       <int>     <num>     <num>\n1:         1 0.9494620 0.9276402\n2:         2 0.9507662 0.9048240\n3:         3 0.9527379 0.9054142\n```\n\n\n:::\n:::\n\n\nAnd on average over the resampling iterations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrr$aggregate(traintest_acc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nacc_train  acc_test \n0.9509887 0.9126261 \n```\n\n\n:::\n:::\n\n\nAs expected, our learner performed better on the training data than on the test data.\nBy default, `{mlr3}` only gives us the results for the test data, which we'll be\nfocusing on going forward.\nBeing able to compare train-/test-performance is useful though to make sure you're\nnot hopelessly overfitting on your training data!\n\nAlso note how we got to train our learner, do cross validation and get scores\nall without having to do any extra work.\nThat's why we use `{mlr3}` instead of doing everything manually --- abstraction is nice.\n\nAlso note that we don't always have to choose a measure, there's a default measure for classification for example.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrr$aggregate()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclassif.ce \n0.08737388 \n```\n\n\n:::\n:::\n\n\n## One more thing: Measures\n\nSo far we've always used the accuracy, i.e. the proportion of correct classifications\nas our measure. For a problem such as spam detection that might not be the best choice,\nbecause it might be better to consider the **probability** that an e-mail is spam\nand maybe adjust the **threshold** at which we start rejecting mail.  \nFor a class prediction we might say that if `prob(is_spam) > 0.5` the message is classified\nas spam, but maybe we'd rather be more conservative and only consider a message to \nbe spam at a probability over, let's say, 70%. This will change the relative amounts of true and false positives and negatives:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn_learner <- lrn(\"classif.kknn\", predict_type = \"prob\", k = 13)\n\nset.seed(123)\nspam_split <- partition(spam_task)\nknn_learner$train(spam_task, spam_split$train)\nknn_pred <- knn_learner$predict(spam_task, spam_split$test)\n\n# Measures: True Positive Rate (Sensitivity) and True Negative Rate (1 - Specificity)\nmeasures <- msrs(c(\"classif.tpr\", \"classif.tnr\"))\nknn_pred$confusion\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         truth\nresponse  spam nonspam\n  spam     508      43\n  nonspam   95     872\n```\n\n\n:::\n\n```{.r .cell-code}\nknn_pred$score(measures)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclassif.tpr classif.tnr \n  0.8424544   0.9530055 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Threshold of 70% probability of spam:\nknn_pred$set_threshold(0.7)\nknn_pred$confusion\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         truth\nresponse  spam nonspam\n  spam     446      20\n  nonspam  157     895\n```\n\n\n:::\n\n```{.r .cell-code}\nknn_pred$score(measures)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclassif.tpr classif.tnr \n  0.7396352   0.9781421 \n```\n\n\n:::\n\n```{.r .cell-code}\n# If we were happy with only 20% probability of spam to mark a message as spam:\nknn_pred$set_threshold(0.2)\nknn_pred$confusion\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         truth\nresponse  spam nonspam\n  spam     570     161\n  nonspam   33     754\n```\n\n\n:::\n\n```{.r .cell-code}\nknn_pred$score(measures)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclassif.tpr classif.tnr \n  0.9452736   0.8240437 \n```\n\n\n:::\n:::\n\n\nFor a better analysis than just manually trying out different thresholds, we can use [ROC curves](https://en.wikipedia.org/wiki/Receiver_operating_characteristic).\n\nTo do so, we first have to adjust our Learner to predict the spam *probability*\ninstead of already simplifying the prediction to `\"spam\"` or `\"nonspam\"`.\nWe use the `autoplot()` function based on the prediction, specifying `type = \"roc\"` to give us an ROC curve:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(knn_pred, type = \"roc\")\n```\n\n::: {.cell-output-display}\n![](02-resampling-rf-boosting_files/figure-html/knn-roc-1.png){width=672}\n:::\n:::\n\n\nThis gives us the false positive rate (FPR, \"1 - Specificity\") and the\nSensitivity (or true positive rate, TPR) for our binary classification example.\n\"Positive\" here means \"the e-mail is spam\". If our classification model was\nbasically a random coin flip, we would expect the curve to be the diagonal (depicted\nin grey in the plot). Everything in the upper-left is at least better than random.\n\nTo condense this to a single number, we use the AUC, the *area under the (ROC) curve*.\nIf this AUC is 0.5, our model is basically a coin toss --- and if it's 1, that\nmeans our model is *perfect*, which is usually too good to be true and means\nwe overfit in some way or the data is weird.\n\nTo get the AUC we use `msr(\"classif.auc\")` instead of `\"classif.acc\"` going forward.\n\n## Your Turn!\n\n1. Repeat the same resampling steps for the `{rpart}` decision tree learner.\n  (Resample with 5-fold CV, evaluate based on test accuracy)\n  - Does it fare better than kNN with default parameters?\n2. Repeat either learner resampling with different hyperparameters\n3. `{mlr3viz}` provides alternatives to the ROC curve, which are described in the help page `?autoplot.PredictionClassif`. \n    Experiment with precision recall and threshold curves. Which would you consider most useful?\n\nThis is technically peck-and-find hyperparameter tuning which we'll do in a\nmore convenient (and methodologically sound) way in the next part :)\n\n## Benchmarking\n\nThe next thing we'll try is to resample across multiple learners at once --- because doing resampling for each learner separately and comparing results is just too tedious.\n\nLet's set up our learners with default parameters and compare them against\na dummy *featureless* learner, which serves as a naive baseline. This learner always predicts the average of the target or the majority class in this case, so it's the worst possible learner that should be beatable by any reasonable algorithm!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlearners <- list(\n  lrn(\"classif.kknn\", id = \"knn\", predict_type = \"prob\"),\n  lrn(\"classif.rpart\", id = \"tree\", predict_type = \"prob\"),\n  lrn(\"classif.featureless\", id = \"Baseline\", predict_type = \"prob\")\n) \n\n# Define task, learners and resampling strategy in a benchmark design\ndesign <- benchmark_grid(\n  tasks = spam_task,       # Still the same task\n  learners = learners,     # The new list of learners\n  resamplings = rsmp(\"cv\", folds = 3)  # Same resampling strategy as before\n) \n\n# Run the benchmark and save the results (\"BenchmarkResult\" object)\nbmr <- benchmark(design)\nbmr\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n── <BenchmarkResult> of 9 rows with 3 resampling run ───────────────────────────\n nr task_id learner_id resampling_id iters warnings errors\n  1    spam        knn            cv     3        0      0\n  2    spam       tree            cv     3        0      0\n  3    spam   Baseline            cv     3        0      0\n```\n\n\n:::\n\n```{.r .cell-code}\n# It's just a collection of ResampleResult objects from before!\nas.data.table(bmr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                                  uhash               task\n                                 <char>             <list>\n1: a6c0569e-9afc-4b8e-9cb8-f33c2c942939 <TaskClassif:spam>\n2: a6c0569e-9afc-4b8e-9cb8-f33c2c942939 <TaskClassif:spam>\n3: a6c0569e-9afc-4b8e-9cb8-f33c2c942939 <TaskClassif:spam>\n4: 8a4b677a-dc10-4399-843c-45885390b44d <TaskClassif:spam>\n5: 8a4b677a-dc10-4399-843c-45885390b44d <TaskClassif:spam>\n6: 8a4b677a-dc10-4399-843c-45885390b44d <TaskClassif:spam>\n7: 2a0ea29d-0ce3-474f-984a-8a6647e23b55 <TaskClassif:spam>\n8: 2a0ea29d-0ce3-474f-984a-8a6647e23b55 <TaskClassif:spam>\n9: 2a0ea29d-0ce3-474f-984a-8a6647e23b55 <TaskClassif:spam>\n                                learner     resampling iteration\n                                 <list>         <list>     <int>\n1:             <LearnerClassifKKNN:knn> <ResamplingCV>         1\n2:             <LearnerClassifKKNN:knn> <ResamplingCV>         2\n3:             <LearnerClassifKKNN:knn> <ResamplingCV>         3\n4:           <LearnerClassifRpart:tree> <ResamplingCV>         1\n5:           <LearnerClassifRpart:tree> <ResamplingCV>         2\n6:           <LearnerClassifRpart:tree> <ResamplingCV>         3\n7: <LearnerClassifFeatureless:Baseline> <ResamplingCV>         1\n8: <LearnerClassifFeatureless:Baseline> <ResamplingCV>         2\n9: <LearnerClassifFeatureless:Baseline> <ResamplingCV>         3\n            prediction task_id learner_id resampling_id\n                <list>  <char>     <char>        <char>\n1: <PredictionClassif>    spam        knn            cv\n2: <PredictionClassif>    spam        knn            cv\n3: <PredictionClassif>    spam        knn            cv\n4: <PredictionClassif>    spam       tree            cv\n5: <PredictionClassif>    spam       tree            cv\n6: <PredictionClassif>    spam       tree            cv\n7: <PredictionClassif>    spam   Baseline            cv\n8: <PredictionClassif>    spam   Baseline            cv\n9: <PredictionClassif>    spam   Baseline            cv\n```\n\n\n:::\n:::\n\n\n`bmr` contains everything we'd like to know about out comparison. We extract the scores\nand take a look:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbm_scores <- bmr$score(msr(\"classif.auc\"))\nbm_scores\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      nr task_id learner_id resampling_id iteration     prediction_test\n   <int>  <char>     <char>        <char>     <int>              <list>\n1:     1    spam        knn            cv         1 <PredictionClassif>\n2:     1    spam        knn            cv         2 <PredictionClassif>\n3:     1    spam        knn            cv         3 <PredictionClassif>\n4:     2    spam       tree            cv         1 <PredictionClassif>\n5:     2    spam       tree            cv         2 <PredictionClassif>\n6:     2    spam       tree            cv         3 <PredictionClassif>\n7:     3    spam   Baseline            cv         1 <PredictionClassif>\n8:     3    spam   Baseline            cv         2 <PredictionClassif>\n9:     3    spam   Baseline            cv         3 <PredictionClassif>\n   classif.auc\n         <num>\n1:   0.9574852\n2:   0.9651055\n3:   0.9574079\n4:   0.9083518\n5:   0.8961702\n6:   0.8849686\n7:   0.5000000\n8:   0.5000000\n9:   0.5000000\nHidden columns: uhash, task, learner, resampling\n```\n\n\n:::\n\n```{.r .cell-code}\n# Extract per-iteration accuracy per learner (only first five rows shows)\nbm_scores[1:5, .(learner_id, iteration, classif.auc)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   learner_id iteration classif.auc\n       <char>     <int>       <num>\n1:        knn         1   0.9574852\n2:        knn         2   0.9651055\n3:        knn         3   0.9574079\n4:       tree         1   0.9083518\n5:       tree         2   0.8961702\n```\n\n\n:::\n\n```{.r .cell-code}\n# To get all results for the tree learner\nbm_scores[learner_id == \"tree\", .(learner_id, iteration, classif.auc)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   learner_id iteration classif.auc\n       <char>     <int>       <num>\n1:       tree         1   0.9083518\n2:       tree         2   0.8961702\n3:       tree         3   0.8849686\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or the results of the first iteration\nbm_scores[iteration == 1, .(learner_id, iteration, classif.auc)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   learner_id iteration classif.auc\n       <char>     <int>       <num>\n1:        knn         1   0.9574852\n2:       tree         1   0.9083518\n3:   Baseline         1   0.5000000\n```\n\n\n:::\n:::\n\n\nAnd if we want to see what worked best overall:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbmr$aggregate(msr(\"classif.auc\"))[, .(learner_id, classif.auc)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   learner_id classif.auc\n       <char>       <num>\n1:        knn   0.9599996\n2:       tree   0.8964969\n3:   Baseline   0.5000000\n```\n\n\n:::\n\n```{.r .cell-code}\nautoplot(bmr, measure = msr(\"classif.auc\"))\n```\n\n::: {.cell-output-display}\n![](02-resampling-rf-boosting_files/figure-html/bm-results-aggregate-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(bmr, type = \"roc\")\n```\n\n::: {.cell-output-display}\n![](02-resampling-rf-boosting_files/figure-html/bm-roc-1.png){width=672}\n:::\n:::\n\n\nWe see what we'd expect regarding the featureless learner --- it's effectively a coin toss. \nAlso, kNN does quite a bit better than the decision tree with the default \nparameters here.\n\nOf course including the featureless learner here doesn't really add any insights,\nespecially since we evaluate by AUC, where the featureless learner gets a score of 0.5 by definition.\n\n## Your Turn!\n\nSince we have a binary classification problem, we might even get away with using\nplain old logistic regression.\n\nInstead of benchmarking against the featureless learner, compare kNN and decision trees\nto the logistic regression learner `\"classif.log_reg\"` without any hyperparameters.\n\nDo our fancy ML methods beat the good old GLM?\nUse the best hyperparameter settings for kNN and `rpart` you have found so far\n\n# Random Forests & Boosting\n\nArmed with our new model comparison skills, we can add Random Forests and\nGradient Boosting to the mix!\n\nOur new learner IDs are\n\n- `\"classif.ranger\"` for Random Forest, see `?ranger::ranger`\n- `\"classif.xgboost\"` for (eXtreme) Gradient Boosting, see `?xgboost::xgboost`\n\n(You know it has to be fancy if it has \"extreme\" in the name!)\n\nBoth learners can already do fairly well without tweaking hyperparameters, except\nfor the `nrounds` value in `xgboost` which sets the number of boosting iterations.\nThe default in the `{mlr3}` learner is 1, which kind of defeats the purpose of boosting.\n\n## Your Turn!\n\nUse the benchmark setup from above and switch the `kknn` and `rpart` learners\nwith the Random Forest and Boosting learners.\n\nMaybe switch to holdout resampling to speed the process up a little and make \nsure to set `nrounds` to something greater than 1 for `xgboost`.\n\nWhat about now? Can we beat logistic regression?\n",
    "supporting": [
      "02-resampling-rf-boosting_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}