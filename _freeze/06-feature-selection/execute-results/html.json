{
  "hash": "98462ce706e97f700b110b17d5fbea0a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Feature Selection\"\neditor_options: \n  chunk_output_type: console\n---\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse) # All the mlr3 things\nlgr::get_logger(\"mlr3\")$set_threshold(\"error\")\n\n# Spam Task setup\nspam_task <- tsk(\"spam\")\n```\n:::\n\n\nGoals of this part:\n\n1. Introduce feature selection\n2. Introduce the `auto_fselector` analogous to `auto_tuner`\n\n# Feature Selection\n\nThere is a lot more to cover than we have time for here, see e.g.:\n\n- [mlr3gallery post with example](https://mlr-org.com/gallery/optimization/2020-09-14-mlr3fselect-basic/index.html)\n- [mlr3book](https://mlr3book.mlr-org.com/chapters/chapter6/feature_selection.html)\n\nSelecting features with `{mlr3}` is similar to parameter tuning: We need to set a budget (e.g. 20 evaluations like before) and a criterion (like the AUC) with a resampling strategy (here holdout for simplicity).\n\nThe feature seelction instance defines our search:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfselect_instance = fsi(\n  task = spam_task,\n  learner = lrn(\"classif.rpart\", predict_type = \"prob\"),\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.auc\"),\n  terminator = trm(\"evals\", n_evals = 20)\n)\n\nfselect_instance\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> <FSelectInstanceBatchSingleCrit>\n#> * State:  Not optimized\n#> * Objective: <ObjectiveFSelectBatch:classif.rpart_on_spam>\n#> * Terminator: <TerminatorEvals>\n```\n\n\n:::\n:::\n\n\nThere are multiple feature selection methods available:\n\n- Random Search (`\"random_search`): Randomly try combinations of features until our budget is exhausted\n- Exhaustive Search (`exhaustive_search`): Try all possible subsets of features. Can take a trillion years. Or 10 minutes\n- Sequential Search (`sequential`): Forwards- (default) or backwards-selection\n- Recursive Feature Elimination (`rfe`): Recursively eliminates features with low `$importance` score (if the `Learner` supports it!)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(mlr_fselectors)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Key: <key>\n#>                       key                         label\n#>                    <char>                        <char>\n#> 1:          design_points                 Design Points\n#> 2:      exhaustive_search             Exhaustive Search\n#> 3:         genetic_search                Genetic Search\n#> 4:          random_search                 Random Search\n#> 5:                    rfe Recursive Feature Elimination\n#> 6:                  rfecv Recursive Feature Elimination\n#> 7:             sequential             Sequential Search\n#> 8: shadow_variable_search        Shadow Variable Search\n#>                             properties           packages\n#>                                 <list>             <list>\n#> 1: dependencies,single-crit,multi-crit  mlr3fselect,bbotk\n#> 2:              single-crit,multi-crit        mlr3fselect\n#> 3:                         single-crit mlr3fselect,genalg\n#> 4:              single-crit,multi-crit        mlr3fselect\n#> 5:          single-crit,requires_model        mlr3fselect\n#> 6:          single-crit,requires_model        mlr3fselect\n#> 7:                         single-crit        mlr3fselect\n#> 8:                         single-crit        mlr3fselect\n```\n\n\n:::\n:::\n\n\nAs you might be able to imagine, doing an exhaustive search is not often feasible when we're working with a lot of features. For a dataset with 10 features, examining every possible subset of features would yield over 1000 models to evaluate. \nYou can imagine how feasible that approach would be for genome-wide studies with thousands of variables.\n\nRandom search it is, then!\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfselector <- fs(\"random_search\")\n\nfselector$optimize(fselect_instance)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>    address addresses    all business capitalAve capitalLong capitalTotal\n#>     <lgcl>    <lgcl> <lgcl>   <lgcl>     <lgcl>      <lgcl>       <lgcl>\n#> 1:   FALSE      TRUE   TRUE     TRUE       TRUE        TRUE         TRUE\n#>    charDollar charExclamation charHash charRoundbracket charSemicolon\n#>        <lgcl>          <lgcl>   <lgcl>           <lgcl>        <lgcl>\n#> 1:      FALSE           FALSE    FALSE             TRUE         FALSE\n#>    charSquarebracket conference credit     cs   data direct    edu  email\n#>               <lgcl>     <lgcl> <lgcl> <lgcl> <lgcl> <lgcl> <lgcl> <lgcl>\n#> 1:              TRUE      FALSE  FALSE  FALSE   TRUE  FALSE   TRUE   TRUE\n#>      font   free george     hp    hpl internet    lab   labs   mail   make\n#>    <lgcl> <lgcl> <lgcl> <lgcl> <lgcl>   <lgcl> <lgcl> <lgcl> <lgcl> <lgcl>\n#> 1:   TRUE   TRUE   TRUE   TRUE  FALSE     TRUE  FALSE  FALSE   TRUE   TRUE\n#>    meeting  money num000 num1999  num3d num415 num650  num85 num857  order\n#>     <lgcl> <lgcl> <lgcl>  <lgcl> <lgcl> <lgcl> <lgcl> <lgcl> <lgcl> <lgcl>\n#> 1:    TRUE   TRUE  FALSE   FALSE  FALSE   TRUE   TRUE   TRUE  FALSE  FALSE\n#>    original    our   over  parts people     pm project     re receive remove\n#>      <lgcl> <lgcl> <lgcl> <lgcl> <lgcl> <lgcl>  <lgcl> <lgcl>  <lgcl> <lgcl>\n#> 1:     TRUE   TRUE   TRUE   TRUE  FALSE   TRUE   FALSE  FALSE    TRUE   TRUE\n#>    report  table technology telnet   will    you   your\n#>    <lgcl> <lgcl>     <lgcl> <lgcl> <lgcl> <lgcl> <lgcl>\n#> 1:   TRUE   TRUE       TRUE   TRUE   TRUE  FALSE   TRUE\n#>                                                          features n_features\n#>                                                            <list>      <int>\n#> 1: addresses,all,business,capitalAve,capitalLong,capitalTotal,...         36\n#>    classif.auc\n#>          <num>\n#> 1:   0.9202298\n```\n\n\n:::\n:::\n\n\nHere we have picked an selection strategy (ultimitaley also just an optimization problem) and used it on our selection problem.\n\nWe can look at the results, also similar to tuning results:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfselect_instance$result_feature_set\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  [1] \"addresses\"         \"all\"               \"business\"         \n#>  [4] \"capitalAve\"        \"capitalLong\"       \"capitalTotal\"     \n#>  [7] \"charRoundbracket\"  \"charSquarebracket\" \"data\"             \n#> [10] \"edu\"               \"email\"             \"font\"             \n#> [13] \"free\"              \"george\"            \"hp\"               \n#> [16] \"internet\"          \"mail\"              \"make\"             \n#> [19] \"meeting\"           \"money\"             \"num415\"           \n#> [22] \"num650\"            \"num85\"             \"original\"         \n#> [25] \"our\"               \"over\"              \"parts\"            \n#> [28] \"pm\"                \"receive\"           \"remove\"           \n#> [31] \"report\"            \"table\"             \"technology\"       \n#> [34] \"telnet\"            \"will\"              \"your\"\n```\n\n\n:::\n\n```{.r .cell-code}\nfselect_instance$result_y\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> classif.auc \n#>   0.9202298\n```\n\n\n:::\n:::\n\n\nWe can also look at the (somewhat unqieldy) tuning archive which shows us all of the feature combinations that we tried out, wehre `TRUE` indicates features that were in this particular evaluation and `FALSE` for those omitted.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(fselect_instance$archive)[1:5, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>    address addresses    all business capitalAve capitalLong capitalTotal\n#>     <lgcl>    <lgcl> <lgcl>   <lgcl>     <lgcl>      <lgcl>       <lgcl>\n#> 1:    TRUE      TRUE   TRUE     TRUE       TRUE        TRUE         TRUE\n#> 2:   FALSE     FALSE  FALSE    FALSE      FALSE       FALSE        FALSE\n#> 3:    TRUE     FALSE   TRUE    FALSE       TRUE       FALSE         TRUE\n#> 4:    TRUE      TRUE  FALSE    FALSE       TRUE        TRUE         TRUE\n#> 5:   FALSE     FALSE  FALSE    FALSE      FALSE       FALSE        FALSE\n#>    charDollar charExclamation charHash charRoundbracket charSemicolon\n#>        <lgcl>          <lgcl>   <lgcl>           <lgcl>        <lgcl>\n#> 1:       TRUE            TRUE     TRUE             TRUE          TRUE\n#> 2:       TRUE           FALSE    FALSE             TRUE         FALSE\n#> 3:      FALSE           FALSE     TRUE            FALSE          TRUE\n#> 4:      FALSE           FALSE     TRUE            FALSE         FALSE\n#> 5:      FALSE           FALSE    FALSE            FALSE         FALSE\n#>    charSquarebracket conference credit     cs   data direct    edu  email\n#>               <lgcl>     <lgcl> <lgcl> <lgcl> <lgcl> <lgcl> <lgcl> <lgcl>\n#> 1:              TRUE       TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE\n#> 2:             FALSE       TRUE  FALSE  FALSE  FALSE  FALSE  FALSE   TRUE\n#> 3:              TRUE       TRUE  FALSE   TRUE   TRUE  FALSE  FALSE   TRUE\n#> 4:              TRUE       TRUE  FALSE   TRUE  FALSE  FALSE   TRUE  FALSE\n#> 5:             FALSE      FALSE  FALSE   TRUE  FALSE  FALSE  FALSE  FALSE\n#>      font   free george     hp    hpl internet    lab   labs   mail   make\n#>    <lgcl> <lgcl> <lgcl> <lgcl> <lgcl>   <lgcl> <lgcl> <lgcl> <lgcl> <lgcl>\n#> 1:   TRUE   TRUE   TRUE   TRUE   TRUE     TRUE   TRUE  FALSE   TRUE   TRUE\n#> 2:  FALSE  FALSE   TRUE   TRUE  FALSE    FALSE  FALSE  FALSE  FALSE  FALSE\n#> 3:  FALSE  FALSE  FALSE   TRUE  FALSE     TRUE  FALSE   TRUE  FALSE  FALSE\n#> 4:  FALSE   TRUE  FALSE   TRUE   TRUE     TRUE  FALSE  FALSE  FALSE   TRUE\n#> 5:  FALSE  FALSE  FALSE   TRUE  FALSE    FALSE   TRUE   TRUE  FALSE  FALSE\n#>    meeting  money num000 num1999  num3d num415 num650  num85 num857  order\n#>     <lgcl> <lgcl> <lgcl>  <lgcl> <lgcl> <lgcl> <lgcl> <lgcl> <lgcl> <lgcl>\n#> 1:    TRUE   TRUE   TRUE    TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE\n#> 2:   FALSE  FALSE  FALSE   FALSE  FALSE   TRUE  FALSE  FALSE  FALSE  FALSE\n#> 3:    TRUE   TRUE  FALSE   FALSE  FALSE   TRUE   TRUE  FALSE   TRUE   TRUE\n#> 4:   FALSE   TRUE  FALSE    TRUE   TRUE  FALSE   TRUE   TRUE  FALSE   TRUE\n#> 5:   FALSE   TRUE  FALSE   FALSE   TRUE  FALSE  FALSE  FALSE  FALSE  FALSE\n#>    original    our   over  parts people     pm project     re receive remove\n#>      <lgcl> <lgcl> <lgcl> <lgcl> <lgcl> <lgcl>  <lgcl> <lgcl>  <lgcl> <lgcl>\n#> 1:     TRUE   TRUE   TRUE   TRUE   TRUE  FALSE    TRUE   TRUE    TRUE   TRUE\n#> 2:    FALSE  FALSE  FALSE  FALSE   TRUE  FALSE   FALSE   TRUE   FALSE  FALSE\n#> 3:    FALSE   TRUE  FALSE  FALSE   TRUE  FALSE   FALSE   TRUE   FALSE  FALSE\n#> 4:    FALSE   TRUE   TRUE  FALSE   TRUE  FALSE    TRUE   TRUE    TRUE  FALSE\n#> 5:    FALSE   TRUE  FALSE  FALSE  FALSE  FALSE   FALSE  FALSE   FALSE  FALSE\n#>    report  table technology telnet   will    you   your classif.auc\n#>    <lgcl> <lgcl>     <lgcl> <lgcl> <lgcl> <lgcl> <lgcl>       <num>\n#> 1:   TRUE   TRUE       TRUE   TRUE   TRUE  FALSE   TRUE   0.9054711\n#> 2:  FALSE  FALSE      FALSE  FALSE   TRUE  FALSE  FALSE   0.9002972\n#> 3:  FALSE  FALSE       TRUE   TRUE  FALSE  FALSE  FALSE   0.8629631\n#> 4:   TRUE   TRUE       TRUE  FALSE   TRUE  FALSE   TRUE   0.8813560\n#> 5:  FALSE  FALSE      FALSE   TRUE  FALSE   TRUE  FALSE   0.8194007\n#>    runtime_learners           timestamp batch_nr warnings errors\n#>               <num>              <POSc>    <int>    <int>  <int>\n#> 1:            0.032 2025-07-15 19:21:04        1        0      0\n#> 2:            0.010 2025-07-15 19:21:04        1        0      0\n#> 3:            0.021 2025-07-15 19:21:04        1        0      0\n#> 4:            0.018 2025-07-15 19:21:04        1        0      0\n#> 5:            0.009 2025-07-15 19:21:04        1        0      0\n#>                                                              features\n#>                                                                <list>\n#> 1:          address,addresses,all,business,capitalAve,capitalLong,...\n#> 2:         charDollar,charRoundbracket,conference,email,george,hp,...\n#> 3:     address,all,capitalAve,capitalTotal,charHash,charSemicolon,...\n#> 4: address,addresses,capitalAve,capitalLong,capitalTotal,charHash,...\n#> 5:                                     cs,hp,lab,labs,money,num3d,...\n#>    n_features  resample_result\n#>        <list>           <list>\n#> 1:         54 <ResampleResult>\n#> 2:         10 <ResampleResult>\n#> 3:         25 <ResampleResult>\n#> 4:         32 <ResampleResult>\n#> 5:          9 <ResampleResult>\n```\n\n\n:::\n:::\n\n\nSimilar to the `auto_tuner` we used for parameter tuning, there's also an `auto_fselector` which basically works the same way, giving us a \"self-tuning\" learner as a result\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfselected_rpart <- auto_fselector(\n  learner = lrn(\"classif.rpart\", predict_type = \"prob\"),\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.ce\"),\n  terminator = trm(\"evals\", n_evals = 20),\n  fselector = fs(\"random_search\")\n)\n\nfselected_rpart\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> <AutoFSelector:classif.rpart.fselector>\n#> * Model: list\n#> * Packages: mlr3, mlr3fselect, rpart\n#> * Predict Type: prob\n#> * Feature Types: logical, integer, numeric, factor, ordered\n#> * Properties: importance, missings, multiclass, selected_features,\n#>   twoclass, weights\n```\n\n\n:::\n:::\n\n\nAnd of course it should be worth it to compare our variable-selected learner with a learner that uses all variables, just to make sure we're not wasting our time:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndesign <- benchmark_grid(\n  tasks = spam_task,\n  learners = list(\n    fselected_rpart,\n    lrn(\"classif.rpart\", predict_type = \"prob\")\n  ),\n  resamplings = rsmp(\"cv\", folds = 3)\n)\n\nbmr <- benchmark(design)\nbmr$aggregate(msr(\"classif.auc\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>       nr task_id              learner_id resampling_id iters classif.auc\n#>    <int>  <char>                  <char>        <char> <int>       <num>\n#> 1:     1    spam classif.rpart.fselector            cv     3   0.8923973\n#> 2:     2    spam           classif.rpart            cv     3   0.8981915\n#> Hidden columns: resample_result\n```\n\n\n:::\n:::\n\n\nOf course this is essentially another form of tuning, and doings feature selection with untuned learners is not going to give you the best possible performance in each iteration, but it gives you a good set of features to start your actual hyperparameter tuning with.\n\n\n## Your turn! (For some other time)\n\n- Try out the bike sharing task (`tsk(\"bike_sharing\")`)\n- [Read the docs](https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset) to see the meaning of each feature\n- Try out different feature selection approaches!\n\nNote that this task has a few more observations, so it's going to take a bit longer.  \nWe don't want to spend the in-person session staring overheating laptops, so you can try this out in your own time!\n",
    "supporting": [
      "06-feature-selection_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}