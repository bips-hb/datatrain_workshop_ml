---
title: "Tuning"
editor_options: 
  chunk_output_type: console
---

```{r setup}
#| message: false
library(mlr3verse) # All the mlr3 things
library(ggplot2) # For plotting
lgr::get_logger("mlr3")$set_threshold("error")

# Spam Task setup
spam_task <- tsk("spam")
set.seed(26)

# train/test split
spam_split <- partition(spam_task, ratio = 2 / 3)
```

Goals of this part:

1.  Introduce hyperparameter tuning
2.  Experiment with tuning different learners

# Hyperparameter Tuning

So far we've seen four learners:

1.  kNN via `{kknn}`
2.  Decision Trees via `{rpart}`
3.  Random Forest via `{ranger}`
4.  Gradient Boosting via `{xgboost}`

We've gotten to know the first two a little, and now we'll also take a closer look at the second two.

First we'll start doing some tuning with `{mlr3}` based on the **kNN** learner because it's nice and simple.
We saw that `k` is an important parameter, and it's an integer greater than 1 at least. To tune it, we also have to make a few other decisions, also using what we learned about (nested) resampling.

-   What's our resampling strategy?
-   What measure to we tune on?
-   What does the parameter search space look like?
-   How long to we tune? What's our *budget*?
-   What's our tuning strategy?

We'll use `{mlr3}`'s [`auto_tuner`](https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html) for this because it's just so convenient:

```{r knn-tuning-setup}
# Defining a search space: k is an integer, we look in range 3 to 51
search_space_knn = ps(
  k = p_int(lower = 3, upper = 51)
)

tuned_knn <- auto_tuner(
  # The base learner we want to tune, optionally setting other parameters
  learner = lrn("classif.kknn", predict_type = "prob"),
  # Resampling strategy for the tuning (inner resampling)
  resampling = rsmp("cv", folds = 3),
  # Tuning measure: Maximize the classification accuracy
  measure = msr("classif.bbrier"),
  # Setting the search space we defined above
  search_space = search_space_knn,
  # Budget: Try n_evals different values
  terminator = trm("evals", n_evals = 20),
  # Strategy: Randomly try parameter values in the space
  # (not ideal in this case but in general a good place to start)
  tuner = tnr("random_search")
)

# Take a look at the new tuning learner
tuned_knn
```

Now `tuned_knn` behaves the same way as any other Learner that has not been trained on any data yet --- first we have to train (and tune!) it on our spam training data. The result will be the best hyperparameter configuration of those we tried:

```{r tune-knn}
# If you're on your own machine with multiple cores available, you can parallelize:
future::plan("multisession", workers = 4)

# Setting a seed so we get the same results -- train on train set only!
set.seed(2398)
tuned_knn$train(spam_task, row_ids = spam_split$train)
```

(Side task: Try the same but with the AUC measure --- do you get the same `k`?)

We can visualize the performance across all values of `k` we tried by accessing the tuning instance now included in the `tuned_knn` learner object:

```{r plot-knn-tuning-result}
autoplot(tuned_knn$tuning_instance)

tuned_knn$tuning_instance
```

(See also docs at `?mlr3viz:::autoplot.TuningInstanceSingleCrit`)

And we can get the hyperparameter results that worked best in the end:

```{r knn-tuning-param-results}
tuned_knn$tuning_result
```

Now that we've tuned on the training set, it's time to evaluate on the test set:

```{r knn-eval}
tuned_knn_pred <- tuned_knn$predict(spam_task, row_ids = spam_split$test)

# Accuracy and AUC
tuned_knn_pred$score(msrs(c("classif.acc", "classif.auc", "classif.bbrier")))
```

That was basically the manual way of doing nested resampling with an inner resampling strategy (CV) and an outer resampling strategy of holdout (the `spam_train` and `spam_test` sets).

In the next step we're going to compare the knn learner with the decision tree learner, and for that we need a proper nested resampling:

```{r knn-nested-resampling}
# Set up the knn autotuner again
tuned_knn <- auto_tuner(
  # The base learner we want to tune, optionally setting other parameters
  learner = lrn("classif.kknn", predict_type = "prob"),
  # Resampling strategy for the tuning (inner resampling)
  resampling = rsmp("cv", folds = 3),
  # Tuning measure: Maximize the classification accuracy
  measure = msr("classif.bbrier"),
  # Setting the search space we defined above
  search_space = ps(k = p_int(lower = 3, upper = 51)),
  # Budget: Try n_evals different values
  terminator = trm("evals", n_evals = 20),
  # Strategy: Randomly try parameter values in the space
  # (not ideal in this case but in general a good place to start)
  tuner = tnr("random_search")
)

# Set up resampling with the ready-to-be-tuned learner and outer resampling: CV
knn_nested_tune <- resample(
  task = spam_task,
  learner = tuned_knn,
  resampling = rsmp("cv", folds = 3), # this is effectively the outer resampling
  store_models = TRUE
)

# Extract inner tuning results, since we now tuned in multiple CV folds
# Folds might conclude different optimal k
# AUC is averaged over inner folds
extract_inner_tuning_results(knn_nested_tune)

# Above combined individual results also accessible via e.g.
knn_nested_tune$learners[[2]]$tuning_result

# Plot of inner folds
autoplot(knn_nested_tune$learners[[1]]$tuning_instance)
autoplot(knn_nested_tune$learners[[2]]$tuning_instance)
autoplot(knn_nested_tune$learners[[3]]$tuning_instance)

# Get AUC and acc for outer folds
knn_nested_tune$score(msrs(c("classif.acc", "classif.auc")))[, .(
  iteration,
  classif.auc,
  classif.acc
)]

# Average over outer folds - our "final result" performance for kNN
# AUC is averaged over outer folds
knn_nested_tune$aggregate(msrs(c("classif.acc", "classif.auc")))
```

Seems like a decent result?
Let's try to beat it with some other learner!

## Your Turn!

Above you have a boilerplate to tune your own learner.
Start with either of the other three learners we've seen, pick one ore two hyperparameters to tune with a reasonable budget (note we have limited time and resources), tune on the training set and evaluate per AUC on the test set.

Some pointers:

-   Consult the Learner docs to see tuning-worthy parameters:

    -   `lrn("classif.xgboost")$help()` links to the `xgboost` help
    -   `lrn("classif.rpart")$help()` analogously for the decision tree
    -   You can also see the documentation online, e.g. <https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.xgboost.html>

-   Parameter search spaces in `ps()` have different types, see the help at `?paradox::Domain`
    -   Use `p_int()` for integers, `p_dbl()` for real-valued params etc.

-   If you don't know which parameter to tune, try the following:
    -   `classif.xgboost`:
        -   Important: `nrounds` (integer) (>= 1 (at least 50 or so))
        -   Important: `eta` (double) (0 < eta < 1 (close to 0 probably))
        -   Maybe: `max_depth` (integer)

    -   `classif.rpart`:
        -   `cp` (double)
        -   Maybe: `maxdepth` (integer) (< 30)

    -   `classif.ranger`:
        -   `mtry` (integer) -> tune `mtry.ratio` (0 < `mtry.ratio` < 1)
        -   `max.depth` (integer)

Note: Instead of randomly picking parameters from the design space, we can also generate a grid of parameters and try those.
We'll not try that here for now, but you can read up on how to do that here: `?mlr_tuners_grid_search`.

-> `generate_design_grid(search_space, resolution = 5)`

Also note that the cool thing about the `auto_tuner()` is that it behaves just like any other `mlr3` learner, but it automatically tunes itself.
You can plug it into `resample()` or `benchmark_grid()` just fine!


```{r tuning-your-turn}
# your code
```


{{< include solutions/04-1-tuning.qmd >}}


::: {.callout-tip title="Not sure what to tune?"}

Which parameter to tune in which interval is usually an area of research, unless you have specific theory- or domain-driven constraints.  

The [mlr3tuningspaces](https://mlr3tuningspaces.mlr-org.com/) packages aims to collect comprehensive tuning spaces for many learners, and when in doubt, they are usually a good place to start.
Note that it's most likely not necessary to extensively tune all parameters as much as possible --- diminishing returns and such!

:::
