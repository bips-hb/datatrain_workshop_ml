---
title: "01: KNN classification"
date: "`r Sys.time()`"
output: 
  html_notebook: 
    toc: yes
    theme: flatly
    number_sections: yes
---

```{r setup}
library(ggplot2)        # For plotting
theme_set(theme_minimal())


library(kknn)           # For kNN models
```

# The dataset: Pengiuns!

See [their website](https://allisonhorst.github.io/palmerpenguins/) for some more
information if your interested. 
For now it's enough to know that we have a bunch of data about 3 species of penguins.

![](https://github.com/allisonhorst/palmerpenguins/raw/master/man/figures/lter_penguins.png)

```{r penguins}
library(palmerpenguins) # For penguins

# remove NAs for simplicity in this example
# (handling missing data is a can of worms for another time :)
penguins <- na.omit(penguins)

str(penguins)
```

We can take a look at the different species across two numeric features: Bill length and depth.

![](https://github.com/allisonhorst/palmerpenguins/raw/master/man/figures/culmen_depth.png)

```{r penguins-plot}
library(ggplot2)        # For plotting
theme_set(theme_minimal()) # Setting a default theme

ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +
  geom_point() +
  labs(
    title = "Palmer Penguins: Species by bill size",
    x = "Bill Length [mm]", y = "Bill Depth [mm]",
    color = "Species"
  )
```
We split our penguin data in a roughly 2/3 to 1/3 training- and test dataset for out first experiments:

```{r penguin-split-manual}
penguin_N <- nrow(penguins) # Our total sample size

# We draw indices randomly with our sampling proportion with a fixed seed
set.seed(234)
train_ids <- sample(penguin_N, replace = FALSE, size = penguin_N * 2/3)
# Our test set are the indices not in the training set
test_ids <- setdiff(1:penguin_N, train_ids)

# Assemble our train/test set using the indices we just randomized
penguin_train <- penguins[train_ids, ]
penguins_test <- penguins[test_ids, ]
```

# kNN, manually

No that we have some data, we can start fitting a kNN model, just to see how it goes!
Given the plot from earlier, we may have a rough idea that the bill- and flipper 
measurements are already giving us a pretty good picture about the species.

```{r}
knn_penguins <- kknn(
  formula = species ~ bill_length_mm + bill_depth_mm,
  k = 3,
  train = penguins_train,
  test = penguins_test
)

# The model seems pretty confident
head(summary(knn_penguins))
```

```{r knn-prediction-check}
# Add predictions to the test dataset
penguins_test$knn_predicted_species <- fitted(knn_penguins)

# Rows: True species, columns: predicted species
table(penguins_test$species, penguins_test$knn_predicted_species)

# Proportion of correct prediction (accuracy)
sum(penguins_test$species == penguins_test$knn_predicted_species) / nrow(penguins_test)
```

## Your turn!

Above you have working code for an oddly accurate kNN model.  
Can you make it even better?
Can you change something to make it *worse*?

Tweak some of the following:
  - Try different predictors, maybe leave some out, or use all of them (`formula = species ~ .`)
  - Try different `k` values. Is higher == better? (You can stick to odd numbers)

# Switching to mlr3

```{r setup-mlr3}
library(mlr3)           # For the basics
library(mlr3learners)   # For learners
library(mlr3viz)        # For plotting
```

Our code above works (hopefully), but for any given model or algorithm, there's
different R packages with slightly different interfaces. `mlr3` and add-on packages
unify all the common tasks with a consistent interface, 

## Creating a task

The task encapsulates our data, including which variables we're using to learn 
and which we want to predict. Tasks can be created in [multiple ways and some standard example tasks are available in mlr3](https://mlr3book.mlr-org.com/tasks.html), but we're taking the long way around.

Questions a task object answers:

- What kind of prediction are we doing?
    - Here: Classification (instead of e.g. regression)
- What are we predicting on?
    - The dataset given to `backend`
- What variable are we predicting?
    - The `target` variable, here `species`
- Which variables are we using for predictions?
    - The `feature` variables

```{r penguins-task-creation}
# Creating a classification task from our penguin data
penguin_task <- TaskClassif$new(
  id = "penguins", 
  backend = penguins, 
  target = "species"
)

# Contains our penguin dataset
penguin_task$data()
# We can ask it about e.g. our sample size
penguin_task$nrow

# Display feature and target variable assignment
penguin_task$col_roles[c("feature", "target")]

# Maybe not all variables are useful for this task
penguin_task$set_col_roles(cols = c("island", "sex", "year"), remove_from = "feature")
penguin_task$col_roles[c("feature", "target")]
```

Some variables may have missing values - if we had not excluded them in the beginning, you'd find them here:

```{r penguins-task-missings}
penguin_task$missings()
```

Always check for missing values to avoid unpleasant surprises!  
Knowing your data is the first step to doing anything useful with it :)


## Creating a learner

We'll use the `kknn` package in the background for the classification task.  
So first, we have to find the learner we're looking for.  
There's a lot more [about learners in the mlr3 book](https://mlr3book.mlr-org.com/learners.html), but for now we're happy with the basics.

```{r learners, eval=FALSE}
# Lots of learners to choose from here:
mlr_learners

# But we only want classifiers for now
mlr_learners$keys(pattern = "classif")

# Or more specifically, the knn one
mlr_learners$keys(pattern = "knn")
```

Now that we've identified our learner, we can get it quickly via the `lrn()` helper function:

```{r knn-learner}
knn_learner <- lrn("classif.kknn")

# What parameters does this learner have?
knn_learner$param_set$ids()

# Setting k to 7 just to mix it up
knn_learner$param_set$values <- list(k = 7)
```

## Train and test split

We're mixing things up with a new train/test split, just for completeness in the example.
For mlr3, we only need to save the indices.

```{r penguin-split}
set.seed(26)
penguin_train <- sample(penguin_task$nrow, 2/3 * penguin_task$nrow)
penguin_test <- setdiff(seq_len(penguin_task$nrow), penguin_train)
```

# Training and evaluating

We can train the learner with default parameters once to see if it works as we expect it to.  

```{r knn-train-once}
knn_learner$train(penguin_task, row_ids = penguin_train)
knn_learner$model
```
And we can make predictions:

```{r knn-predict-once}
penguins_prediction <- knn_learner$predict(penguin_task, row_ids = penguin_test)
penguins_prediction
```

Our predictions are looking quite good.

```{r knn-confusion-matrix}
penguins_prediction$confusion
```

```{r knn-accuracy-once}
# Saving the accuracy measure, we'll use it a bunch
measure_acc <- msr("classif.acc")
# Scores according to the selected measure
penguins_prediction$score(measure_acc)
```

Rule of thumb: Perfect classification accuracy is _too good to be true_.  
Always.  
Mostly.  
In general.

## Resampling

There's [lots of resampling strategies](https://mlr3book.mlr-org.com/resampling.html), but you usually can't go too wrong with cross validation (CV), as long as the scope is fairly small at least.

Instead of training our model just once, we're going to train it 5 times, which should give us a better idea of the accuracy we can expect:

```{r knn-resample-cv}
resampling <- rsmp("cv", folds = 5)
rr <- resample(penguin_task, knn_learner, resampling)
```

Now we have a look at our predictions again, per resampling iteration:

```{r knn-resample-acc}
rr$score(measure_acc)[, .(iteration, classif.acc)]
```

And on average over the resampling iterations:

```{r knn-resample-acc-avg}
rr$aggregate(measure_acc)

autoplot(rr, measure = measure_acc) +
  coord_flip()
```


Much more realistic. Still confusingly good!

Also, note how we got to train our learner, do cross validate and get accuracy scores
all without having to do any extra work.
That's why we use mlr3 instead of doing everything manually - abstraction is nice.

## Comparing against a baseline

```{r}
learners <- list(
  lrn("classif.kknn", id = "knn"),
  lrn("classif.rpart", id = "tree"),
  lrn("classif.featureless", id = "Baseline")
) 

design <- benchmark_grid(
  tasks = penguin_task,    # Still the same task
  learners = learners,     # The new list of learners
  resamplings = resampling # Same resampling strategy as before
) 

# Run the benchmark and save the results
bmr <- benchmark(design)
```

`bmr` contains all we'd like to know about out comparison, let's take a look:

```{r}
# Extract per-iteration accuracy per learner
bmr$score(measure_acc)[1:5, .(learner_id, iteration, classif.acc)]
```

And if we want to see what worked better overall:

```{r}
bmr$aggregate(measure_acc)[, .(learner_id, classif.acc)]
```

## Comparing more learners

We haven't touched decision trees yet, but mlr3 makes it really easy to add them to the mix!

This is the classification tree learner from the `rpart` package:

```{r}
lrn("classif.rpart")
```

