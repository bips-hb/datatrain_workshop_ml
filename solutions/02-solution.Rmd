---
title: "02: Resampling, Random Forest & Boosting"
date: "`r Sys.time()`"
output: 
  html_notebook: 
    toc: yes
    theme: flatly
    number_sections: yes
editor_options: 
  chunk_output_type: console
---

```{r setup}
library(mlr3verse)
library(ggplot2)

lgr::get_logger("mlr3")$set_threshold("error")
lgr::get_logger("bbotk")$set_threshold("error")
```

# Task Setup

## Your turn!

## Train & Test split

# Resampling

## One more thing: Measures

## Your Turn!

## Example Code

The idea would be to re-run this code chunk with different hyperparameters:

```{r example-rr-rpart}
rr <- resample(
  task = spam_task, 
  # Important: set predict_type to "prob", set other parameters as desired.
  learner = lrn("classif.rpart", predict_type = "prob", maxdepth = 15, cp = 0.003),
  resampling = rsmp("cv", folds = 3)
)

rr$score(msr("classif.acc"))[, .(classif.acc)]
```

ROC curve based on resampling iterations:

```{r}
autoplot(rr, type = "roc")
rr$aggregate(msr("classif.auc"))
```

Alternatives to ROC:
Precision-Recall curve (prc) and a threshold-error curve --- all three can be very useful depending on your specific classification problem!

```{r knn-roc}
autoplot(rr, type = "prc")

# Threshold plot doesn't work on resampling result, but on prediction object!
autoplot(rr$prediction(), type = "threshold")
```

## Benchmarking

## Your Turn!

## Example Code

```{r example-bm-setup-logreg}
learners <- list(
  lrn("classif.kknn", id = "knn", predict_type = "prob", k = 25),
  lrn("classif.rpart", id = "tree", predict_type = "prob", maxdepth = 11, cp = 0.0036),
  lrn("classif.log_reg", id = "LogReg", predict_type = "prob")
) 

design <- benchmark_grid(
  tasks = spam_task,       # Still the same task
  learners = learners,     # The new list of learners
  resamplings = rsmp("cv", folds = 3)  # Same resampling strategy as before
) 

# Run the benchmark and save the results
bmr <- benchmark(design)

autoplot(bmr, type = "roc")
bmr$aggregate(msr("classif.auc"))[, .(learner_id, classif.auc)]
```


# Random Forests & Boosting

## Your Turn!

## Example Code

```{r example-benchmark-rf-boosting-logreg}
learners <- list(
  lrn("classif.ranger", id = "forest", predict_type = "prob"),
  lrn("classif.xgboost", id = "xgboost", predict_type = "prob", nrounds = 5),
  lrn("classif.log_reg", id = "LogReg", predict_type = "prob")
) 

design <- benchmark_grid(
  tasks = spam_task,       # Still the same task
  learners = learners,     # The new list of learners
  resamplings = rsmp("cv", folds = 3)
) 

# Run the benchmark and save the results
bmr <- benchmark(design)

autoplot(bmr, type = "roc")
bmr$aggregate(msr("classif.auc"))[, .(learner_id, classif.auc)]
```
