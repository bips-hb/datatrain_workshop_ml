---
title: '01: kNN & Trees'
date: "`r Sys.time()`"
output:
  html_notebook:
    toc: yes
    theme: flatly
    number_sections: yes
editor_options: 
  chunk_output_type: console
---

```{r setup}
library(ggplot2)        # For plotting
library(palmerpenguins) # For penguins
library(kknn)           # For kNN models
library(rpart)          # For decision trees
```

# kNN and trees, step by step

## kNN

## Your turn!

## Example Code

```{r example-knn-penguins}
# Already perfect accuracy with multiple predictors, not so interesting
knn_penguins <- kknn(
  formula = species ~ flipper_length_mm + body_mass_g + bill_length_mm + bill_depth_mm,
  k = 5,
  train = penguins_train,
  test = penguins_test
)

penguins_test$knn_predicted_species <- fitted(knn_penguins)
mean(penguins_test$species == penguins_test$knn_predicted_species)
```


The "try a bunch of `k`"-shortcut function (using less features to be moderately interesting):

```{r example-knn-penguins-looptune}
knn_try_k <- function(k) {
  knn_penguins <- kknn::kknn(
    formula = species ~ flipper_length_mm + body_mass_g + bill_depth_mm,
    k = k,
    train = penguins_train,
    test = penguins_test
  )

  penguins_test$knn_predicted_species <- fitted(knn_penguins)
  acc <- mean(penguins_test$species == penguins_test$knn_predicted_species)
  
  data.frame(k = k, accuracy = acc)
}

# Call function ^ with k = 1 through 10, collect result as data.frame
k_result <- do.call(rbind, lapply(1:10, knn_try_k))

ggplot(k_result, aes(x = k, y = accuracy)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = 1:10) +
  scale_y_continuous(labels = scales::percent) +
  labs(
    title = "kNN test accuracy on penguin dataset",
    x = "k", y = "Test accuracy"
  ) +
  theme_minimal() +
  theme(panel.grid.minor = element_blank())
```

This is a cumbersome way to find the "best" `k` though --- we'll learn about the better ways later!

## Growing a decision tree

## Your turn!

Not much to show here, just fiddle around with `rpart` a little!

## Plotting decision boundaries (for 2 predictors)

# Introducing `{mlr3}`

## Creating a task

### Train and test split

## Picking a Learner

## Training and evaluating

## Your turn!

## Example Code

```{r example-rpart-learner}
# Picking the learner
rpart_learner <- lrn("classif.rpart")

# What parameters does this learner have?
rpart_learner$param_set$ids()

# Setting parameters (omit to use the defaults)
rpart_learner$param_set$values <- list(maxdepth = 15)

# Train
rpart_learner$train(penguin_task, row_ids = penguin_split$train)

# Predict
rpart_prediction <- rpart_learner$predict(penguin_task, row_ids = penguin_split$test)

rpart_prediction$confusion

# Accuracy
rpart_prediction$score(msr("classif.acc"))

plot_learner_prediction(rpart_learner, penguin_task)
```

# Useful links
