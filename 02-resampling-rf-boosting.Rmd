---
title: "02: Random Forest & Boosting"
date: "`r Sys.time()`"
output: 
  html_notebook: 
    toc: yes
    theme: flatly
    number_sections: yes
---

```{r setup}
library(mlr3verse) # Loads all the mlr3 stuff
library(ggplot2)   # For plotting

# Just telling mlr3 to be quiet unless something broke
lgr::get_logger("mlr3")$set_threshold("error")
```

# Task Setup

As we've seen last time, our penguin data is pretty easy for our learners.
We need something a little more complex, meaning more observations (n)
and a couple more predictors (p).  
If you're looking for ready-made example tasks for your own experimentation,
mlr3 comes with a couple you can try. The procedure is similar to `mlr_learners`
and `mlr_measures`, but this time it's, you guessed it,  

```{r task-dictionary}
mlr_tasks

# As a neat little table with all the relevant info
as.data.table(mlr_tasks)
```

I suggest we try a two-class (i.e. binary) classification problem next, and
maybe something we can probably all somewhat relate to: **Spam detection**.

```{r task-spam}
spam_task <- tsk("spam")

# Outcome categories
spam_task$class_names
```

## Your turn!

Explore the task a little (you can use its built-in methods `spam_task$...`)
to check the data, get column types, the dataset dimensions etc.
THink about this as a new analysis problem, so what do we need to know here?

(If you'd like a simpler overview, you can just read the help with `spam_task$help()`)

## Train & Test split

Not to forget our obligatory train- and test split:

```{r spam-split}
set.seed(26)
spam_train <- sample(spam_task$nrow, 2/3 * spam_task$nrow)
spam_test <- setdiff(seq_len(spam_task$nrow), spam_train)
```

# Resampling

To continue with our kNN and tree experiments, we'll now enter resampling territory.  
You probably realized that predictiong on just one test dataset doesn't give us
too much of a useful idea of our model, which is why we use resampling.

There's [lots of resampling strategies](https://mlr3book.mlr-org.com/resampling.html), 
but you usually can't go too wrong with cross validation (CV), which should not 
take too much time and computing power on small datasets like ours.

Instead of training our model just once, we're going to train it 5 times with 
different training- and test-datasets. In mlr3, we first pick a resampling
strategy with `rsmp()` and that define out setup with `resample()` based on
our task and resampling strategy:

```{r knn-resample-cv}
# 5-fold CV
resampcv5 <- rsmp("cv", folds = 5)

rr <- resample(
  task = spam_task, 
  learner = lrn("classif.kknn", k = 13), # Optional: Adjust learner parameters
  resampling = resampcv5
)
```

Now we have a look at our predictions, but *per resampling iteration*:

```{r knn-resample-acc}
rr$score(msr("classif.acc"))[, .(iteration, classif.acc)]
```

And on average over the resampling iterations:

```{r knn-resample-acc-avg}
rr$aggregate(msr("classif.acc"))
```

Note how we got to train our learner, do cross validate and get scores
all without having to do any extra work.
That's why we use mlr3 instead of doing everything manually - abstraction is nice.

## One more thing: Measures

So far we've always used the accuracy, i.e. the proportion of correct classifications
as our measure. For a problem such as spam detection that might not be the best choice,
because it might be better to consider the **probability** that an e-mail is spam
and maybe adjust the **threshold** at which we start rejecting mail.  
For a class prediction we might say that if `prob(is_spam) > 0.5` the message is classified
as spam, but maybe we'd rather be more conservative and only consider a message to 
be spam at a probability over, let's say, 95%.

For that, we can use [ROC curves](https://en.wikipedia.org/wiki/Receiver_operating_characteristic).

To do so, we first have to adjust our Learner to predict the spam *probability*
instead of already simplifying the prediction to `"spam"` or `"nonspam"`.
We re-do out resampling set-up and use the `autoplot()` function based on the
prediction, specifying `type = "roc"` to give us an ROC curve:

```{r knn-roc}
rr <- resample(
  task = spam_task, 
  # Important: set predict_type to "prob", set other parameters as desired.
  learner = lrn("classif.kknn", predict_type = "prob", k = 13),
  resampling = resampcv5
)

autoplot(rr$prediction(), type = "roc")
```
This gives us the false positive rate (FPR, 1 - Specificity) and the
Sensitivity (or true positive rate, TPR) for our binary classification example.
"Positive" here means "the e-mail is spam". If our classification model was
basically a random coin flip, we would expect the curve to be the diagonal (depicted
in grey in the plot). Everything in the upper-left is at least better than random.

To condense this to a single number, we use the AUC, the *area under the (ROC) curve*.
If this AUC is 0.5, our model is basically a coin toss - and if it's 1, that
means our model is *perfect*, which is usually too good to be true and means
we probably overfit.

To get the AUC we use `msr("classif.auc")` instead of `"classif.acc"` going forward.

## Comparing against a baseline

Let's set up our learners with default parameters and compare them against
a dummy *featureless* learner, which serves as a naive baseline.

```{r benchmark-grid}
learners <- list(
  lrn("classif.kknn", id = "knn", predict_type = "prob"),
  lrn("classif.rpart", id = "tree", predict_type = "prob"),
  lrn("classif.featureless", id = "Baseline", predict_type = "prob")
) 

design <- benchmark_grid(
  tasks = spam_task,       # Still the same task
  learners = learners,     # The new list of learners
  resamplings = resampcv5  # Same resampling strategy as before
) 

# Run the benchmark and save the results
bmr <- benchmark(design)
```

`bmr` contains all we'd like to know about out comparison, let's take a look:

```{r rr-results-full}
# Extract per-iteration accuracy per learner
bmr$score(msr("classif.auc"))[1:5, .(learner_id, iteration, classif.auc)]

# To get all results for the tree learner
bmr$score(msr("classif.auc"))[learner_id == "tree", .(learner_id, iteration, classif.auc)]

# Or the results of the first iteration
bmr$score(msr("classif.auc"))[iteration == 1, .(learner_id, iteration, classif.auc)]
```

And if we want to see what worked better overall:

```{r rr-results-aggregate}
bmr$aggregate(msr("classif.auc"))[, .(learner_id, classif.auc)]

autoplot(bmr, measure = msr("classif.auc"))
```

We see what we'd expect regarding the featureless learner - it's the same as
a coin toss. 
Also, kNN does quite a bit better than the decision tree with the default 
parameters here.

Of course including the featureless learner here doesn't really add any insights,
especially since its results shouldn't change throughout resampling iterations.
Sometimes it can't hurt to confirm your assumptions though.

# Random Forests & Boosting

Armed with our new model comparison skills, we can add Random Forests and
Gradient Boosting to the mix!

```{r}
lrn_ranger <- lrn("classif.ranger", predict_type = "prob")

lrn_ranger$train(spam_task, row_ids = spam_train)

spam_pred_ranger <- lrn_ranger$predict(spam_task, row_ids = spam_test)

spam_pred_ranger$score(msr("classif.acc"))
autoplot(spam_pred_ranger, type = "roc")
```

```{r}
lrn_xgboost <- lrn("classif.xgboost", predict_type = "prob")

lrn_xgboost$train(spam_task, row_ids = spam_train)

spam_pred_xgboost <- lrn_xgboost$predict(spam_task, row_ids = spam_test)

spam_pred_xgboost$score(msr("classif.acc"))
```


```{r}
autoplot(spam_pred_xgboost, type = "roc")

spam_pred_xgboost$score(msr("classif.auc"))
```

## Comparing all methods

```{r}
learners <- list(
  lrn("classif.kknn", id = "knn", predict_type = "prob"),
  lrn("classif.rpart", id = "tree", predict_type = "prob"),
  lrn("classif.ranger", id = "forest", predict_type = "prob"),
  lrn("classif.xgboost", id = "boost", predict_type = "prob")
) 

design <- benchmark_grid(
  tasks = spam_task,       # Still the same task
  learners = learners,     # The new list of learners
  resamplings = resampcv5  # Same resampling strategy as before
) 

# Run the benchmark and save the results
bmr <- benchmark(design)
```

```{r}
autoplot(bmr, measure = measure_auc)
```

