---
title: '01: kNN & Trees'
date: "`r Sys.time()`"
output:
  html_notebook:
    toc: yes
    theme: flatly
    number_sections: yes
  html_document:
    toc: yes
    df_print: paged
---

```{r setup}
library(ggplot2)        # For plotting
theme_set(theme_minimal())

library(kknn)           # For kNN models
```

Goals of this part:

  1. Taking a look at our example dataset
  2. Introduce kNN via `{kknn}` and decision trees via `{rpart}`
  3. Train some models, look at some results
  4. Introduce `{mlr3}` and do 3. again but nicer

# The dataset: Pengiuns!

See [their website](https://allisonhorst.github.io/palmerpenguins/) for some more
information if your interested. 
For now it's enough to know that we have a bunch of data about 3 species of penguins.

![](https://github.com/allisonhorst/palmerpenguins/raw/master/man/figures/lter_penguins.png)

```{r penguins}
library(palmerpenguins) # For penguins

# remove NAs for simplicity in this example
# (handling missing data is a can of worms for another time :)
penguins <- na.omit(penguins)

str(penguins)
```

We can take a look at the different species across two numeric features: Bill length and depth.

![](https://github.com/allisonhorst/palmerpenguins/raw/master/man/figures/culmen_depth.png)

```{r penguins-plot}
library(ggplot2)        # For plotting
theme_set(theme_minimal()) # Setting a default theme

ggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +
# ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +
  geom_point() +
  labs(
    title = "Palmer Penguins",
    x = "Flipper Length [mm]", y = "Body Mass [g]",
    color = "Species"
  )
```
We split our penguin data in a roughly 2/3 to 1/3 training- and test dataset for out first experiments:

```{r penguin-split-manual}
penguin_N <- nrow(penguins) # Our total sample size

# We draw indices randomly with our sampling proportion with a fixed seed
set.seed(234)
train_ids <- sample(penguin_N, replace = FALSE, size = penguin_N * 2/3)
# Our test set are the indices not in the training set
test_ids <- setdiff(1:penguin_N, train_ids)

# Assemble our train/test set using the indices we just randomized
penguins_train <- penguins[train_ids, ]
penguins_test <- penguins[test_ids, ]
```

# kNN and trees, step by step

Now that we have some data, we can start fitting models, just to see how it goes!
Given the plot from earlier, we may have a rough idea that the flipper length and body mass
measurements are already giving us a somewhat decent good picture about the species.

## kNN

Let's start with the nearest-neighbor approach via the `{kknn}` package.
It takes a `formula` argument like you may know from `lm()` and other common modeling functions in R,
where the format is `predict_this ~ on_this + and_that + ...`.

```{r kknn-fit}
library(kknn)
knn_penguins <- kknn(
  formula = species ~ flipper_length_mm + body_mass_g,
  k = 3,                  # Hyperparameter: How many neighbors to consider
  train = penguins_train, # Training data used to make predictions
  test = penguins_test    # Date to make predictions on
)

# Peek at the predictions
head(summary(knn_penguins))
```

```{r knn-prediction-check}
# Add predictions to the test dataset
penguins_test$knn_predicted_species <- fitted(knn_penguins)

# Rows: True species, columns: predicted species
table(penguins_test$species, penguins_test$knn_predicted_species)

# Proportion of correct prediction (accuracy)
mean(penguins_test$species == penguins_test$knn_predicted_species)
```

## Your turn!

Above you have working code for an acceptable but *not great* kNN model.  
Can you make it even better?
Can you change something to make it *worse*?

Some things to try:

  1. Try different predictors, maybe leave some out
      - Which seems to work best?
  2. Try using all available predictors (`formula = species ~ .`)
      - Would you recommend doing that? Does it work well?
  3. Try different `k` values. Is higher == better? (You can stick to odd numbers)
      - After you've tried a couple `k`'s, does it get cumbersome yet?


## Growing a decision tree

Now that we've played around with kNN a little, let's grow some trees!
We'll use the `{rpart}` (**R**ecursive **Part**itioning) package and start
with the same model specification as before and use the default parameters.

```{r tree-fit}
library(rpart)

rpart_penguins <- rpart(
  formula = species ~ flipper_length_mm + body_mass_g,
  data = penguins_train, # Train data
  method = "class"      # Grow a classification tree (don't change this here)
)
```

The nice thing about a single tree is that you can just look at it and know exactly what it did:

```{r tree-model}
rpart_penguins

# Looking at the tree as a... tree.
plot(rpart_penguins)
text(rpart_penguins)

# Much nicer to use the rpart.plot package
library(rpart.plot)
rpart.plot(rpart_penguins)
```

If we cant to know how accurate we are with our model we need to make predictions on our test data manually:

```{r tree-predict}
rpart_predictions <- predict(
  rpart_penguins,          # The model we just fit
  newdata = penguins_test, # New data to predict species on
  type = "class"           # We want class predictions (the species), not probabilities
)

penguins_test$rpart_predicted_species <- rpart_predictions

# Same procedure as with kNN before
table(penguins_test$species, penguins_test$rpart_predicted_species)

# And our accuracy score
mean(penguins_test$species == penguins_test$rpart_predicted_species)
```


## Your turn!

We haven't picked any hyperparameter settings for our tree yet, maybe we should try?

1. What hyperparameters does `rpart()` offer? Do you recognize some from the lecture? 
    - You can check via `?rpart.control`
    - `minsplit`, `maxdepth` and `cp` seem easy enough to tweak, when in doubt, start there
2. Try out trees with different sizes (`max.depth`) and other parameters, including more predictors (`formula =`)
    - Would you prefer simple or complex trees?
    - How far can you improve the tree's accuracy?
    
So, what seems to work better here? kNN or trees?

## Plotting decision boundaries (for 2 predictors)

```{r}
flipper_range <- range(penguins$flipper_length_mm)
mass_range <- range(penguins$body_mass_g)

pred_grid <- expand.grid(
  flipper_length_mm = seq(flipper_range[1], flipper_range[2], length.out = 200),
  body_mass_g = seq(mass_range[1], mass_range[2], length.out = 200)
)

pred_grid$rpart_prediction <- predict(rpart_penguins, newdata = pred_grid, type = "class")

ggplot(pred_grid, aes(x = flipper_length_mm, y = body_mass_g, color = rpart_prediction)) +
  geom_point(alpha = .1) +
  geom_point(data = penguins_train, aes(color = species)) +
  labs(
    title = "Palmer Penguins",
    subtitle = "Species predicted by decision tree",
    x = "Flipper Length [mm]", y = "Body Mass [g]",
    color = "[Predicted] Species"
  )
```

# Introducing `{mlr3}`

```{r setup-mlr3}
library(mlr3)           # For the basics
library(mlr3learners)   # For learners
library(mlr3viz)        # For plotting
```

Now, imagine you want to try out some more hyperparameters for either `rpart()`
or `kknn()` or both, and then you want to compare the two - that would probably
be kind of tedious unless to write some wrapper functions, right?
Well, luckily we're not the first people to do some machine learning!

Our code above works (hopefully), but for any given model or algorithm, there's
different R packages with slightly different interfaces as you've seen. 
`{mlr3}` and add-on packages unify all the common tasks with a consistent interface.

## Creating a task

The task encapsulates our data, including which variables we're using to learn 
and which we want to predict. Tasks can be created in [multiple ways and some standard example tasks are available in mlr3](https://mlr3book.mlr-org.com/tasks.html), but we're taking the long way around.

Questions a task object answers:

- What kind of prediction are we doing?
    - Here: Classification (instead of e.g. regression)
- What are we predicting on?
    - The dataset given to `backend =`
- What variable are we predicting?
    - The `target` variable, here `species`
- Which variables are we using for predictions?
    - The `feature` variables, which we can adjust

```{r penguins-task-creation}
# Creating a classification task from our penguin data
penguin_task <- TaskClassif$new(
  id = "penguins", 
  backend = penguins, 
  target = "species"
)

# Contains our penguin dataset
penguin_task$data()
# We can ask it about e.g. our sample size
penguin_task$nrow
# And what the classes are
penguin_task$class_names
```


```{r penguins-task-modification}
# Display feature and target variable assignment
penguin_task$col_roles[c("feature", "target")]

# Maybe not all variables are useful for this task, let's remove some
penguin_task$set_col_roles(cols = c("island", "sex", "year"), remove_from = "feature")

# Check what our current variables and roles are
penguin_task$col_roles[c("feature", "target")]
```

Some variables may have missing values - if we had not excluded them in the beginning, you'd find them here:

```{r penguins-task-missings}
penguin_task$missings()
```
### Train and test split

We're mixing things up with a new train/test split, just for completeness in the example.
For mlr3, we only need to save the indices.

```{r penguin-split}
set.seed(26)
penguin_train <- sample(penguin_task$nrow, 2/3 * penguin_task$nrow)
penguin_test <- setdiff(seq_len(penguin_task$nrow), penguin_train)
```


## Picking a Learner

A learner encapsulates the fitting algorithm as well as any relevant hyperparameters,
and `{mlr3}` supports a whole lot of learners to choose from.
We'll keep using `{kknn}` and `{rpart}` in the background for the classification task,
but we'll use `{mlr3}` on top of them for a consistent interface.
So first, we have to find the learners we're looking for.  

There's a lot more [about learners in the mlr3 book](https://mlr3book.mlr-org.com/learners.html), 
but for now we're happy with the basics.

```{r learners, eval=FALSE}
# Lots of learners to choose from here:
mlr_learners

# To show all classification learners
mlr_learners$keys(pattern = "classif")

# Or more specifically, the kknn and rpart learners
mlr_learners$keys(pattern = "knn|rpart")
```

Now that we've identified our learners, we can get it quickly via the `lrn()` helper function:

```{r knn-learner}
knn_learner <- lrn("classif.kknn")

# What parameters does this learner have?
knn_learner$param_set$ids()

# Setting parameters leaves the others as default
knn_learner$param_set$values <- list(k = 9)
```

We'll save the `rpart` learner for later.

## Training and evaluating

We can train the learner with default parameters once to see if it works as we expect it to.  

```{r knn-train-once}
knn_learner$train(penguin_task, row_ids = penguin_train)
knn_learner$model
```

And we can make predictions:

```{r knn-predict-once}
knn_prediction <- knn_learner$predict(penguin_task, row_ids = penguin_test)
knn_prediction
```

Our predictions are looking quite good.

```{r knn-confusion-matrix}
knn_prediction$confusion
```

To calculate the prediction accuracy, we don't have to do any math in small steps.
`{mlr3}` comes with lots of measures (like accuracy) we can use, they're organized 
in the `mlr_measures` object (just like `mlr_learners`).
We're using `"classif.acc"` here with the shorthand function `msr()`.
The we score our predictions with this measure, using the `$score()` method.

```{r knn-accuracy-once}
# Saving the accuracy measure
measure_acc <- msr("classif.acc")
# Scores according to the selected measure
knn_prediction$score(measure_acc)
```

## Your turn!

Now that we've done the kNN fitting with `{mlr3}`, you can easily do the same thing
with the `rpart`-learner! All you have to do is switch the learner objects.

Here's the same code as above but compacted for you to play around with,
just make it do the same steps (training, predicting) with the `rpart_learner`.

```{r switch-rpart-learner}
# Picking the learner
knn_learner <- lrn("classif.kknn")

# What parameters does this learner have?
knn_learner$param_set$ids()

# Setting parameters (omit to use a defaults)
knn_learner$param_set$values <- list(k = 15)

# Train
knn_learner$train(penguin_task, row_ids = penguin_train)
# Optional: peek at the model
knn_learner$model

# Predict
knn_prediction <- knn_learner$predict(penguin_task, row_ids = penguin_test)

knn_prediction$confusion

# Accuracy
knn_prediction$score(measure_acc)
```


# Useful links

- [mlr3 cheatsheet with basic syntax](https://cheatsheets.mlr-org.com/mlr3.pdf)
- [mlr3 book](https://mlr3book.mlr-org.com/) (not complete yet but useful)
